{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN436p/xkhkV94jfLRL+0KF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"189ea337ea4e488088bc36b5535dccfe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fcde33f0157742b3bb145a11047b4cd1","IPY_MODEL_61a0f4c028274845872bb2c67f388f83","IPY_MODEL_f80b711abe644ebfa1cef82191e263b0"],"layout":"IPY_MODEL_e53c373301ba4a72a9d34692b8909ceb"}},"fcde33f0157742b3bb145a11047b4cd1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28afef4df1494d7780272115b8425760","placeholder":"​","style":"IPY_MODEL_32694a5b753a46db88ace75fced4de60","value":"Esecuzione Inferenza Veloce: 100%"}},"61a0f4c028274845872bb2c67f388f83":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8336df6bfcb457898873b6d9467481a","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7bf8e3f52cb5429ebfdde1d50cfef0db","value":10}},"f80b711abe644ebfa1cef82191e263b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6eca9acfd5a94e42a8b82e2e8ed9dfe5","placeholder":"​","style":"IPY_MODEL_bcb590872e904198b9a2d4c03ee53a99","value":" 10/10 [00:04&lt;00:00,  2.92it/s]"}},"e53c373301ba4a72a9d34692b8909ceb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28afef4df1494d7780272115b8425760":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32694a5b753a46db88ace75fced4de60":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8336df6bfcb457898873b6d9467481a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bf8e3f52cb5429ebfdde1d50cfef0db":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6eca9acfd5a94e42a8b82e2e8ed9dfe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcb590872e904198b9a2d4c03ee53a99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["Only mountains virtual images are loaded in a drive folder"],"metadata":{"id":"VMzwNMYMy1mK"}},{"cell_type":"code","source":["# we transer them in the virtual env\n","SOURCE_IMAGES_DIR= '/content/drive/MyDrive/virtualonlymountains'\n","UNZIP_PATH = '/content/data_extracted'\n","\n","\n","DEST_IMAGES_DIR = os.path.join(UNZIP_PATH, 'virtualonlymountains')\n","\n","\n","if os.path.exists(SOURCE_IMAGES_DIR):\n","    print(\"\\n2. Copia dati virtuali aggiuntivi (Montagna) \")\n","\n","    # taregt folder must exist\n","    os.makedirs(DEST_IMAGES_DIR, exist_ok=True)\n","\n","\n","    # copying data to dest folder\n","    !cp -R \"{SOURCE_IMAGES_DIR}/\"* \"{DEST_IMAGES_DIR}\"\n","\n","\n","    print(f\"Copia completata. Dati montagna disponibili in: {DEST_IMAGES_DIR}\")\n","else:\n","    print(f\"\\nAVVISO: Cartella virtualonlymountains non trovata in {SOURCE_IMAGES_DIR}\")\n","\n","print(\"\\nTutte le operazioni sui dati sono complete.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u8X8HSX3hk3B","executionInfo":{"status":"ok","timestamp":1764775388608,"user_tz":-60,"elapsed":292459,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"489039c9-0607-4f69-d9c5-73df564a0dd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","2. Copia dati virtuali aggiuntivi (Montagna) in corso...\n","Copia completata. Dati montagna disponibili in: /content/data_extracted/virtualonlymountains\n","\n","Tutte le operazioni sui dati sono complete.\n"]}]},{"cell_type":"code","source":["# Verifying all images are loaded\n","\n","DEST_IMAGES_DIR = '/content/data_extracted/virtualonlymountains'\n","\n","print(f\"Ispezione della directory: {DEST_IMAGES_DIR}\")\n","\n","# Elenca i primi 10 file per confermare che i dati siano presenti\n","print(\"\\nPrimi 10 file copiati:\")\n","!ls {DEST_IMAGES_DIR} | head -n 10\n","\n","# Conta il numero totale di file (per assicurarsi che siano tutti stati copiati)\n","print(\"\\nConteggio totale delle immagini:\")\n","!ls {DEST_IMAGES_DIR} | wc -l\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6o7UZXP4x1E5","executionInfo":{"status":"ok","timestamp":1764775410894,"user_tz":-60,"elapsed":280,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"4bb3fcfd-a7a2-4ef4-bc90-b74c4c11a140"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ispezione della directory: /content/data_extracted/virtualonlymountains\n","\n","Primi 10 file copiati:\n","scene01_000000.jpg\n","scene01_000001.jpg\n","scene01_000002.jpg\n","scene01_000003.jpg\n","scene01_000004.jpg\n","scene01_000005.jpg\n","scene01_000006.jpg\n","scene01_000007.jpg\n","scene01_000008.jpg\n","scene01_000009.jpg\n","\n","Conteggio totale delle immagini:\n","1662\n"]}]},{"cell_type":"markdown","source":["Zip file with train real and virtual ( only mountains ) is loaded"],"metadata":{"id":"UnKtU4WMyw6g"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rPpKkdrXV3CF","executionInfo":{"status":"ok","timestamp":1764775449847,"user_tz":-60,"elapsed":2253,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"f65bf5a2-2e89-4a83-b91a-1f9df0eaa09b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Percorso del file ZIP su Drive: /content/drive/MyDrive/zip_data_cv.zip\n","Percorso di destinazione per la decompressione: /content/data_extracted\n"]}],"source":["from google.colab import drive\n","import os\n","\n","# 1. Monta Google Drive\n","drive.mount('/content/drive')\n","\n","# Definisci il percorso del tuo file ZIP su Drive\n","# DEVI SOSTITUIRE 'My Drive' e 'Drone_Project_Data' con i nomi esatti delle tue cartelle\n","ZIP_PATH_ON_DRIVE = '/content/drive/MyDrive/zip_data_cv.zip'\n","\n","# Definisci la directory dove i dati verranno decompressi (nella memoria temporanea di Colab)\n","UNZIP_PATH = '/content/data_extracted'\n","\n","if not os.path.exists(UNZIP_PATH):\n","    os.makedirs(UNZIP_PATH)\n","\n","print(f\"Percorso del file ZIP su Drive: {ZIP_PATH_ON_DRIVE}\")\n","print(f\"Percorso di destinazione per la decompressione: {UNZIP_PATH}\")"]},{"cell_type":"code","source":["import shutil\n","\n","# Esegui il comando di decompressione\n","# -q: modalità silenziosa\n","# -o: sovrascrivi file esistenti senza chiedere\n","# -d: directory di destinazione\n","print(\"Avvio decompressione. Attendere...\")\n","\n","# Rimuovi la cartella precedente (se esiste) per una pulizia completa\n","if os.path.exists(os.path.join(UNZIP_PATH, 'real_LRDD')):\n","    shutil.rmtree(os.path.join(UNZIP_PATH, 'real_LRDD'))\n","\n","# Esegui la decompressione\n","!unzip -q -o {ZIP_PATH_ON_DRIVE} -d {UNZIP_PATH}\n","\n","print(\"Decompressione completata con successo!\")\n","# Verifica rapida della struttura\n","!ls {UNZIP_PATH}/real_LRDD/train/images | head -n 3\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o1YUOsM3W2uj","executionInfo":{"status":"ok","timestamp":1764776219725,"user_tz":-60,"elapsed":329171,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"83924dca-7854-40f5-ffbb-64d6258d2aa4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Avvio decompressione. Attendere...\n","Decompressione completata con successo!\n","DRONE_001_1005.jpg\n","DRONE_001_1014.jpg\n","DRONE_001_1018.jpg\n"]}]},{"cell_type":"markdown","source":["Verifying new path\n"],"metadata":{"id":"ST6FFD2IajSJ"}},{"cell_type":"code","source":["BASE_DATA_PATH = UNZIP_PATH\n","\n","virt_img_dir = f\"{BASE_DATA_PATH}/virtual/train/images\"\n","virt_lbl_dir = f\"{BASE_DATA_PATH}/virtual/train/labels\"\n","real_img_dir = f\"{BASE_DATA_PATH}/real_LRDD/train/images\"\n","real_lbl_dir = f\"{BASE_DATA_PATH}/real_LRDD/train/labels\"\n","\n","\n","# Verify paths (sanity check)\n","print(f\"Verified Source Images: {virt_img_dir}\")\n","print(f\"Verified Target Images: {real_img_dir}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-LuTkcOZNXK","executionInfo":{"status":"ok","timestamp":1764775493204,"user_tz":-60,"elapsed":46,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"c636cb58-6330-4cf3-e1e4-c81d1b8b325b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Verified Source Images: /content/data_extracted/virtual/train/images\n","Verified Target Images: /content/data_extracted/real_LRDD/train/images\n"]}]},{"cell_type":"markdown","source":["Verifying the presence of all datasets"],"metadata":{"id":"oPffYx9Jpp15"}},{"cell_type":"code","source":["def check_dataset_presence(base_path):\n","    \"\"\"Controlla se le cartelle chiave esistono e quanti file contengono.\"\"\"\n","\n","    # Percorsi delle cartelle chiave (basati sulla sua configurazione)\n","    paths_to_check = {\n","        \"Source (Virtual) Images\": os.path.join(base_path, 'virtual/train/images'),\n","        \"Source (Virtual) Labels\": os.path.join(base_path, 'virtual/train/labels'),\n","        \"Target (Real) Images\": os.path.join(base_path, 'real_LRDD/train/images'),\n","        \"Target (Real) Labels\": os.path.join(base_path, 'real_LRDD/train/labels'),\n","        \"Virtual Mountains Images (Added)\": os.path.join(base_path, 'virtualmountains/train/images'),\n","    }\n","\n","    print(\"\\n--- RISULTATO DELLA VERIFICA DEI DATI ESTRATTI ---\")\n","\n","    all_ok = True\n","    for name, path in paths_to_check.items():\n","        if os.path.isdir(path):\n","            count = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n","            print(f\" {name}: Trovato ({count} file)\")\n","        else:\n","            print(f\"{name}: NON TROVATO (Percorso mancante: {path})\")\n","            all_ok = False\n","\n","    if all_ok:\n","        print(\"\\nSETUP DATI CONFERMATO: Tutte le directory chiave contengono file.\")\n","    else:\n","        print(\"\\nSETUP DATI FALLITO: Rivedere la struttura del file ZIP principale.\")\n","\n","# ESECUZIONE DELLA VERIFICA\n","# In Colab, le variabili definite nelle celle precedenti sono globali.\n","check_dataset_presence(UNZIP_PATH)"],"metadata":{"id":"hyJssS9A0z14"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Function set up**"],"metadata":{"id":"3-_4K8g0p4rL"}},{"cell_type":"code","source":["# installing torchvision\n","#!pip install torchvision"],"metadata":{"id":"p__omqpiaq4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Function definition, inspired by Domain Adaptive Faster R-CNN for Object Detection in the Wild\" (Chen et al., 2018)\n"],"metadata":{"id":"4bFm9m_2bJZe"}},{"cell_type":"code","source":["import os\n","from glob import glob\n","from typing import List, Dict, Tuple\n","import itertools\n","from collections import OrderedDict\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.autograd as autograd\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.utils as utils # For gradient clipping\n","\n","import torchvision\n","from torchvision.models.detection import (\n","    fasterrcnn_resnet50_fpn,\n","    FasterRCNN_ResNet50_FPN_Weights\n",")\n","from torchvision.models.detection.rpn import AnchorGenerator\n","import torchvision.transforms.functional as TF\n","from PIL import Image"],"metadata":{"id":"FFCXW9e8bN15"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GradientReverseFn(autograd.Function):\n","    \"\"\"Handles the forward (identity) and backward (gradient reversal) pass.\"\"\"\n","    @staticmethod\n","    def forward(ctx, x, lambda_):\n","        # Save the dynamic lambda for the backward pass. This parameter\n","        # controls the strength of the domain adaptation loss.\n","        ctx.lambda_ = lambda_\n","        # Forward pass is just an identity function: output = input\n","        return x.view_as(x)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        #  Reverse and scale the gradient.\n","        # This makes the feature extractor (backbone) learn domain-invariant features,\n","        # as it tries to *maximize* the domain classification loss.\n","        return -ctx.lambda_ * grad_output, None # The 'None' handles the gradient for lambda_\n","\n","#A standard PyTorch Module wrapper for the GRL Function.\n","# This makes it easy to integrate into the model's forward method.\n","class GradientReverse(nn.Module):\n","    \"\"\"Module wrapper for GRL, accepts lambda in forward for dynamic scheduling.\"\"\"\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, lambda_):\n","        # Pass the dynamically calculated lambda_ directly to the function\n","        return GradientReverseFn.apply(x, lambda_)\n","\n","# ==============================================================================\n","# DA-FASTER R-CNN MODEL FOR GLOBAL IMAGE-LEVEL DOMAIN ADAPTATION\n","# Architecture inspired by: \"Domain Adaptive Faster R-CNN for Object Detection in the Wild\" (Chen et al., 2018)\n","\n","class DAFasterRCNN_Global_Stable(nn.Module):\n","    \"\"\"\n","    Domain-Adaptive Faster R-CNN with COCO pre-trained weights initialization\n","    for a stable Task Loss. This version includes image-level domain adaptation.\n","    \"\"\"\n","    def __init__(self, num_classes, lambda_da_max=1.0, da_warmup_steps=4000):\n","        super().__init__()\n","        # Hyperparameters for the Domain Adaptation (DA) component\n","        self.lambda_da_max = lambda_da_max\n","        self.da_warmup_steps = da_warmup_steps\n","        self.current_step = 0 # To track training progress for the warm-up schedule\n","\n","        # --- 1. BASE DETECTOR CONFIGURATION (Faster R-CNN) ---\n","\n","        # Load COCO weights to pre-train the feature extractor (backbone)\n","        weights = FasterRCNN_ResNet50_FPN_Weights.COCO_V1\n","\n","        # Custom anchors for drone detection (or small objects)\n","        # Using smaller sizes is crucial for detecting objects like drones.\n","        anchor_generator = AnchorGenerator(\n","            sizes=((8, 16, 32), (16, 32, 64), (32, 64, 128), (64, 128, 256), (128, 256, 512)),\n","            aspect_ratios=((0.5, 1.0, 2.0),) * 5\n","        )\n","\n","        # MANUAL BACKBONE WEIGHT LOADING TO AVOID RPN/BOX HEAD SIZE MISMATCH ---\n","\n","        # 1. Initialize the detector with the correct configuration (custom anchors, N classes)\n","        # We start without weights because the RPN/Box Head dimensions are different from COCO's\n","        self.detector = fasterrcnn_resnet50_fpn(\n","            weights=None,\n","            num_classes=num_classes, # E.g., background + drone (2 classes)\n","            rpn_anchor_generator=anchor_generator\n","        )\n","\n","        # 2. Create a temporary model only to fetch the pre-trained COCO weights\n","        coco_model = fasterrcnn_resnet50_fpn(weights=weights)\n","\n","        # 3. Copy ONLY the backbone's weights from the COCO model to our model.\n","        # This keeps the feature extraction strong while allowing the RPN and Box Heads\n","        # to correctly initialize for our custom anchor sizes and number of classes.\n","        self.detector.backbone.load_state_dict(coco_model.backbone.state_dict())\n","\n","        # The final layers (RPN Head and Box Predictor) maintain their default initialization\n","        # (usually random) and the correct dimensions.\n","\n","        # --- 2. DOMAIN ADAPTATION (DA) CONFIGURATION ---\n","        self.grl = GradientReverse()\n","        self.im_domain_head = None # Will be lazily initialized based on feature size\n","\n","    def set_current_step(self, step):\n","        \"\"\"Allows external update of the current iteration/step count.\"\"\"\n","        self.current_step = step\n","\n","    def warmup_lambda(self):\n","        \"\"\"Gradually increase lambda_da over first steps (linear schedule).\"\"\"\n","        # A common technique to stabilize early-stage training: the DA loss\n","        # is gradually introduced to allow the detector to learn basic features first.\n","        if self.current_step >= self.da_warmup_steps:\n","            return self.lambda_da_max\n","\n","        progress = self.current_step / self.da_warmup_steps\n","        return progress * self.lambda_da_max\n","\n","    def _build_domain_head(self, feature_dict):\n","        \"\"\"Initializes the domain classifier based on the feature dimensions.\"\"\"\n","        num_scales = len(feature_dict) # Number of FPN levels (usually 5)\n","        # Assuming FPN gives 256-dim features (p2, p3, p4, p5, p6)\n","        feat_dim = next(iter(feature_dict.values())).shape[1]\n","        input_dim = num_scales * feat_dim # Total size after concatenation\n","\n","        # Domain Classifier Head: simple MLP for binary classification (Source vs. Target)\n","        self.im_domain_head = nn.Sequential(\n","            nn.Linear(input_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 2) # Output: Logits for Source (0) and Target (1)\n","        ).to(next(self.parameters()).device)\n","\n","    def forward(self, images, targets=None, domain=None):\n","        if self.training and domain is None:\n","            raise ValueError(\"Need domain='source' or 'target' during training.\")\n","\n","        # Inference mode: standard Faster R-CNN forward pass\n","        if not self.training:\n","            self.detector.eval()\n","            with torch.no_grad():\n","                return self.detector(images)\n","\n","        # --- TRAINING MODE ---\n","\n","        # 1. Feature Extraction (Backbone + FPN)\n","        images_t, targets_t = self.detector.transform(images, targets)\n","        features = self.detector.backbone(images_t.tensors)\n","        # Ensure features are a dictionary for FPN (handle single-tensor case)\n","        if isinstance(features, torch.Tensor):\n","            features = OrderedDict([(\"0\", features)])\n","\n","        # 2. Lazy initialization: Build Domain Head if it hasn't been done yet\n","        if self.im_domain_head is None:\n","            self._build_domain_head(features)\n","\n","        det_losses = {}\n","        # 3. DETECTION TASK LOSS (Only calculated on labeled SOURCE data)\n","        if domain == \"source\":\n","            # Call the standard Faster R-CNN forward to get RPN and Box losses\n","            det_outputs = self.detector.forward(images, targets)\n","            det_losses.update(det_outputs)\n","\n","        # 4. DOMAIN ADAPTATION LOSS (Applied to both Source and Target)\n","        da_losses = {}\n","\n","        # Domain label (0=source, 1=target) for the Cross-Entropy loss\n","        dom_label = 0 if domain == \"source\" else 1\n","        dom_label_tensor = torch.full(\n","            (len(images),), dom_label, dtype=torch.long, device=images[0].device\n","        )\n","\n","        # Global pooled features: Aggregate features from all FPN levels (P2-P5)\n","        pooled = [\n","            # Global Average Pooling (GAP) on each feature map to get a fixed-size vector\n","            F.adaptive_avg_pool2d(f, 1).flatten(1)\n","            for f in features.values()\n","        ]\n","        im_feat = torch.cat(pooled, dim=1) # Concatenate the features: [Batch, Total_Feature_Dim]\n","\n","        # 5. GRL with dynamic warm-up\n","        lambda_da = self.warmup_lambda()\n","        # Apply GRL: gradient is reversed and scaled by lambda_da.\n","        # This makes the backbone learn domain-invariant features.\n","        rev_feat = self.grl(im_feat, lambda_da)\n","\n","        # Classify the domain based on the reversed features\n","        logits = self.im_domain_head(rev_feat)\n","        # Calculate DA loss (Cross-Entropy)\n","        da_losses[\"loss_da_im\"] = F.cross_entropy(logits, dom_label_tensor)\n","\n","        # 6. Combine all losses (Detection Loss + Domain Adaptation Loss)\n","        losses = {}\n","        losses.update(det_losses)\n","        losses.update(da_losses)\n","        return losses"],"metadata":{"id":"IfD71-xMav5I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dataloader"],"metadata":{"id":"TLm7MktUblch"}},{"cell_type":"code","source":["class YoloTxtDetectionDataset(Dataset):\n","    \"\"\"Loads images and converts YOLO format labels (normalized cx,cy,w,h)\n","    to TorchVision format (absolute x1,y1,x2,y2).\"\"\"\n","    def __init__(self, img_dir: str, label_dir: str, transforms=None):\n","        self.img_paths = sorted(\n","            [p for p in glob(os.path.join(img_dir, \"*\")) if p.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n","        )\n","        self.label_dir = label_dir\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def _load_yolo_labels(self, img_path: str, w: int, h: int):\n","        base = os.path.splitext(os.path.basename(img_path))[0]\n","        label_path = os.path.join(self.label_dir, base + \".txt\")\n","\n","        boxes = []\n","        labels = []\n","\n","        if not os.path.exists(label_path):\n","            # Crucial: return empty tensors if no labels found\n","            return torch.empty((0, 4), dtype=torch.float32), torch.empty((0,), dtype=torch.int64)\n","\n","        with open(label_path, \"r\") as f:\n","            for line in f.readlines():\n","                line = line.strip()\n","                if not line: continue\n","                parts = line.split()\n","                # YOLO format: class_id cx cy w h (normalized to 0-1)\n","                cls = int(parts[0])\n","                x_c, y_c, bw, bh = map(float, parts[1:5])\n","\n","                # Convert normalized cx, cy, w, h → absolute pixel xyxy\n","                x_c, y_c, bw, bh = x_c * w, y_c * h, bw * w, bh * h\n","                x1 = x_c - bw / 2\n","                y1 = y_c - bh / 2\n","                x2 = x_c + bw / 2\n","                y2 = y_c + bh / 2\n","\n","                boxes.append([x1, y1, x2, y2])\n","                # IMPORTANT: Faster R-CNN reserves class ID 0 for background.\n","                # If YOLO is 0-indexed, shift classes by 1.\n","                labels.append(cls + 1)\n","\n","        if boxes:\n","            boxes = torch.tensor(boxes, dtype=torch.float32)\n","            labels = torch.tensor(labels, dtype=torch.int64)\n","        else:\n","            boxes = torch.empty((0, 4), dtype=torch.float32)\n","            labels = torch.empty((0,), dtype=torch.int64)\n","\n","        return boxes, labels\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        w, h = img.size\n","\n","        boxes, labels = self._load_yolo_labels(img_path, w, h)\n","\n","        target = {\n","            \"boxes\": boxes,\n","            \"labels\": labels,\n","            \"image_id\": torch.tensor([idx]),\n","            # Required for TorchVision: area and iscrowd\n","            \"area\": (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) if boxes.numel() > 0 else torch.tensor([], dtype=torch.float32),\n","            \"iscrowd\": torch.zeros((boxes.shape[0],), dtype=torch.int64),\n","        }\n","\n","        if self.transforms:\n","            img, target = self.transforms(img, target)\n","\n","        # Convert PIL Image to Tensor\n","        img = TF.to_tensor(img)\n","        return img, target\n","\n","def det_collate(batch):\n","    \"\"\"Custom collate function required by TorchVision detection models.\"\"\"\n","    imgs, targets = zip(*batch)\n","    return list(imgs), list(targets)"],"metadata":{"id":"Y3VSBroHbkm4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Function of the model"],"metadata":{"id":"0J_l3Cqnbxcp"}},{"cell_type":"code","source":["def train_da_stable(\n","    model,\n","    source_loader,\n","    target_loader,\n","    optimizer,\n","    num_epochs=15,\n","    da_weight=0.001,\n","    device=\"cuda\"\n","):\n","    model.train()\n","    # Cycle the target loader so it runs as long as the source loader\n","    target_iter = itertools.cycle(target_loader)\n","    global_step = 0\n","\n","    # Initialize total_loss outside the loop for safe use in checkpointing on epoch 0\n","    total_loss = torch.tensor(0.0, device=device)\n","\n","    print(f\"Starting training on device: {device}\")\n","    print(f\"Total batches per epoch: {len(source_loader)}\")\n","\n","    for epoch in range(num_epochs):\n","\n","        # -----------------------------------------------\n","        # 1. BATCH LOOP (INNER LOOP)\n","        # -----------------------------------------------\n","        for i, (src_imgs, src_tgts) in enumerate(source_loader):\n","\n","            # Update step and lambda schedule\n","            global_step += 1\n","            model.set_current_step(global_step)\n","\n","            src_imgs = [img.to(device) for img in src_imgs]\n","            src_tgts = [{k: v.to(device) for k, v in t.items()} for t in src_tgts]\n","\n","            # SOURCE FORWARD PASS (Task Loss + DA Loss)\n","            src_loss_dict = model(src_imgs, src_tgts, domain=\"source\")\n","\n","            # Aggregate detection task losses\n","            det_loss = (\n","                src_loss_dict.get(\"loss_classifier\", torch.tensor(0.0, device=device)) +\n","                src_loss_dict.get(\"loss_box_reg\", torch.tensor(0.0, device=device)) +\n","                src_loss_dict.get(\"loss_objectness\", torch.tensor(0.0, device=device)) +\n","                src_loss_dict.get(\"loss_rpn_box_reg\", torch.tensor(0.0, device=device))\n","            )\n","            da_src = src_loss_dict.get(\"loss_da_im\", torch.tensor(0.0, device=device))\n","\n","            #  TARGET FORWARD PASS (DA Loss ONLY)\n","            tgt_imgs, _ = next(target_iter)\n","            tgt_imgs = [img.to(device) for img in tgt_imgs]\n","\n","            tgt_loss_dict = model(tgt_imgs, targets=None, domain=\"target\")\n","            da_tgt = tgt_loss_dict.get(\"loss_da_im\", torch.tensor(0.0, device=device))\n","\n","            #  TOTAL LOSS & OPTIMIZATION\n","            # Total Loss = Task Loss + DA Weight * (DA Source Loss + DA Target Loss)\n","            total_loss = det_loss + da_weight * (da_src + da_tgt)\n","\n","            optimizer.zero_grad()\n","            total_loss.backward()\n","\n","            # Gradient clipping for adversarial stability (CRITICAL)\n","            utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n","\n","            optimizer.step()\n","\n","            # LOGGING\n","            if global_step % 10 == 0:\n","                current_lambda = model.warmup_lambda()\n","                print(\n","                    f\"[Epoch {epoch} Step {global_step}] \"\n","                    f\"det={det_loss.item():.4f} \"\n","                    f\"DA_src={da_src.item():.4f} \"\n","                    f\"DA_tgt={da_tgt.item():.4f} \"\n","                    f\"lambda={current_lambda:.6f} \"\n","                    f\"total={total_loss.item():.4f}\"\n","                )\n","\n","        # -----------------------------------------------\n","        # 2. CHECKPOINT SAVING (Runs once per epoch)\n","        # -----------------------------------------------\n","        save_path = f\"da_frcnn_epoch_{epoch}.pth\"\n","        torch.save({\n","            'epoch': epoch,\n","            'global_step': global_step,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': total_loss.item(), # Last calculated loss of this epoch\n","        }, save_path)\n","        print(f\"\\nModel checkpoint saved to {save_path}\\n\")\n","\n","    print(\"Training complete.\")"],"metadata":{"id":"fYMoTQcObZFp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Running it"],"metadata":{"id":"Io5qFTcZeHb5"}},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torch.nn.utils as utils # For gradient clipping\n","\n","# NOTE: The train_da_stable function must be defined in the cell above (Section 3/4)\n","\n","# --- HYPERPARAMETERS ---\n","num_classes = 2          # Background (0) + Drone Class (1)\n","lambda_da_max = 0.0\n","da_warmup_steps = 2000\n","da_weight = 0.00005 #decreased from 0.0001\n","num_epochs = 30\n","batch_size = 2           # Use this small size for stability/memory\n","num_workers = 2\n","\n","\n","\n","\n","#  DEVICE & SETUP\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n","\n","#  DATA LOADERS\n","source_dataset = YoloTxtDetectionDataset(virt_img_dir, virt_lbl_dir)\n","target_dataset = YoloTxtDetectionDataset(real_img_dir, real_lbl_dir)\n","\n","source_loader = DataLoader(\n","    source_dataset, batch_size=batch_size, shuffle=True, collate_fn=det_collate, num_workers=num_workers\n",")\n","target_loader = DataLoader(\n","    target_dataset, batch_size=batch_size, shuffle=True, collate_fn=det_collate, num_workers=num_workers\n",")\n","\n","# MODEL & OPTIMIZER\n","model = DAFasterRCNN_Global_Stable(\n","    num_classes=num_classes,\n","    lambda_da_max=lambda_da_max,\n","    da_warmup_steps=da_warmup_steps\n",").to(device)\n","\n","optimizer = optim.SGD(\n","    model.parameters(),\n","    lr=0.001,\n","    momentum=0.9,\n","    weight_decay=0.0005\n",")\n","\n","#  LAUNCH TRAINING\n","\n","# CRITICAL: We modify the training function here to save directly to Google Drive\n","# NOTE: This assumes the full train_da_stable function is available from Section 4 of your original script.\n","\n","def train_da_stable(\n","    model, source_loader, target_loader, optimizer, num_epochs=10, da_weight=0.001, device=\"cuda\"\n","):\n","    model.train()\n","    target_iter = itertools.cycle(target_loader)\n","    global_step = 0\n","    total_loss = torch.tensor(0.0, device=device)\n","\n","    # Define the save directory on Google Drive\n","    DRIVE_SAVE_DIR = os.path.join(BASE_DRIVE_PATH, \"Checkpoints\")\n","    os.makedirs(DRIVE_SAVE_DIR, exist_ok=True) # Ensure directory exists\n","\n","    #  (Rest of the train_da_stable logic runs as defined in your final script) ...\n","    #\n","    # PASTE THE FULL train_da_stable FUNCTION LOGIC HERE\n","    #  REMEMBER TO MODIFY THE CHECKPOINT SAVE PATH!\n","\n","    # Example modified saving logic:\n","    for epoch in range(num_epochs):\n","        # ... (INNER BATCH LOOP CODE) ...\n","        for i, (src_imgs, src_tgts) in enumerate(source_loader):\n","            # ... (all the forward/backward/optimization logic) ...\n","\n","            # --- LOGGING (Important for Colab, as Colab sessions can be monitored) ---\n","            if global_step % 100 == 0: # Log less frequently for large jobs\n","                # ... print logs ...\n","                pass\n","\n","        # 2. CHECKPOINT SAVING (Saves directly to Google Drive)\n","        save_path = os.path.join(DRIVE_SAVE_DIR, f\"da_frcnn_epoch_{epoch}.pth\")\n","        torch.save({\n","            'epoch': epoch,\n","            'global_step': global_step,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': total_loss.item(),\n","        }, save_path)\n","        print(f\"\\nModel checkpoint saved to Google Drive at {save_path}\\n\")\n","\n","    print(\"Training complete.\")\n","\n","\n","# Launch the training function (make sure the full function body is defined in this cell)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8b4-fA7zeIdo","executionInfo":{"status":"ok","timestamp":1764318426860,"user_tz":-60,"elapsed":1831,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"b2963bbf-0c17-4cbe-d0dd-8daee7a7ad40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using GPU: Tesla T4\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 97.8M/97.8M [00:00<00:00, 148MB/s]\n"]}]},{"cell_type":"markdown","source":["Cell to verify the loss has been saved"],"metadata":{"id":"Mw61QcnIs3-6"}},{"cell_type":"code","source":["\n","# checkpoint path\n","CHECKPOINT_PATH = '/content/drive/MyDrive/Checkpoints/da_frcnn_epoch_1.pth'\n","device = torch.device(\"cuda\")\n","\n","try:\n","    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n","\n","    saved_loss = checkpoint.get('loss', 'N/A')\n","\n","    print(\"--- Analisi Checkpoint Epoca 0 ---\")\n","    print(f\"Epoca salvata: {checkpoint.get('epoch')}\")\n","    print(f\"Global Step salvato: {checkpoint.get('global_step')}\")\n","    print(f\"Loss Totale Registrata (Ultimo Batch): {saved_loss:.4f}\")\n","\n","    if saved_loss > 1.0:\n","        print(\"➡️ LA DET LOSS E' ANCORA ALTA: Il modello non ha imparato molto o la Task Loss è rimasta dominante.\")\n","    else:\n","        print(\"✅ LA DET LOSS E' SCESA: Il modello ha iniziato ad imparare. Procedi con il test di 5 epoche.\")\n","\n","except Exception as e:\n","    print(f\"ERRORE nel caricamento del checkpoint: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1b2ZETA_2KOm","executionInfo":{"status":"ok","timestamp":1764696476690,"user_tz":-60,"elapsed":9136,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"4b561331-d406-46a6-bfe9-6d7129c0ee59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Analisi Checkpoint Epoca 0 ---\n","Epoca salvata: 1\n","Global Step salvato: 0\n","Loss Totale Registrata (Ultimo Batch): 0.0000\n","✅ LA DET LOSS E' SCESA: Il modello ha iniziato ad imparare. Procedi con il test di 5 epoche.\n"]}]},{"cell_type":"code","source":["# Path Configuration\n","\n","BASE_DRIVE_PATH = '/content/drive/MyDrive/'\n","\n","# L'output (checkpoint) verrà salvato qui: /content/drive/MyDrive/Drone_Project_Data/Checkpoints\n","DRIVE_SAVE_DIR = os.path.join(BASE_DRIVE_PATH, \"Checkpoints\")\n","os.makedirs(DRIVE_SAVE_DIR, exist_ok=True)\n","\n","\n","if not os.path.isdir(virt_img_dir):\n","    raise FileNotFoundError(f\"Il percorso delle immagini virtuali non è stato trovato: {virt_img_dir}\")\n","\n","print(f\"I checkpoint verranno salvati in: {DRIVE_SAVE_DIR}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"koXpXkSIluqp","executionInfo":{"status":"ok","timestamp":1764318435322,"user_tz":-60,"elapsed":47,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"e486313c-6770-437b-8883-53616b7ebc0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I checkpoint verranno salvati in: /content/drive/MyDrive/Checkpoints\n"]}]},{"cell_type":"code","source":["\n","\n","train_da_stable(model, source_loader, target_loader, optimizer, num_epochs=num_epochs, da_weight=da_weight, device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0sQiFsZfBcJ","executionInfo":{"status":"ok","timestamp":1764320335358,"user_tz":-60,"elapsed":1891889,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"6b871ed8-61d0-474e-9f21-1c01546e6f80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_0.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_1.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_2.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_3.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_4.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_5.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_6.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_7.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_8.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_9.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_10.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_11.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_12.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_13.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_14.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_15.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_16.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_17.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_18.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_19.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_20.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_21.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_22.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_23.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_24.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_25.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_26.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_27.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_28.pth\n","\n","\n","Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_29.pth\n","\n","Training complete.\n"]}]},{"cell_type":"markdown","source":["Valuation\n"],"metadata":{"id":"R76Kh9f-i134"}},{"cell_type":"code","source":["# Cella Colab per il Caricamento del Modello\n","\n","import torch\n","import os\n","# Importa la classe DAFasterRCNN_Global_Stable (deve essere definita in una cella precedente)\n","\n","# --- CONFIGURAZIONE ---\n","CHECKPOINT_FILENAME = 'da_frcnn_epoch_29.pth'\n","DRIVE_SAVE_DIR = '/content/drive/MyDrive/Checkpoints' # DEVE corrispondere al percorso di salvataggio\n","FINAL_CHECKPOINT_PATH = os.path.join(DRIVE_SAVE_DIR, CHECKPOINT_FILENAME)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","num_classes = 2 # Background + 1 classe Drone\n","\n","# 1. Inizializza il modello (lo scheletro)\n","model = DAFasterRCNN_Global_Stable(\n","    num_classes=num_classes,\n","    # Questi parametri non sono usati in inferenza, ma sono necessari per l'inizializzazione\n","    lambda_da_max=1.0,\n","    da_warmup_steps=1000\n",").to(device)\n","\n","# 2. Carica lo stato del checkpoint\n","print(f\"Caricamento pesi da: {FINAL_CHECKPOINT_PATH}\")\n","checkpoint = torch.load(FINAL_CHECKPOINT_PATH, map_location=device)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# 3. Imposta la modalità di valutazione\n","model.eval()\n","\n","print(\"Modello caricato e impostato in modalità di valutazione (model.eval()). Pronto per il test.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5vdx1B5Ci3zP","executionInfo":{"status":"ok","timestamp":1764320739433,"user_tz":-60,"elapsed":2533,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"85cd7c42-b545-405b-a577-8fa1138ec64d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Caricamento pesi da: /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_29.pth\n","Modello caricato e impostato in modalità di valutazione (model.eval()). Pronto per il test.\n"]}]},{"cell_type":"markdown","source":["Tets dataset"],"metadata":{"id":"8XWVgYfIjSDJ"}},{"cell_type":"code","source":["# Cella Colab per il Caricamento Dati di Test\n","\n","\n","\n","\n","TEST_IMG_DIR = f\"{BASE_DATA_PATH}//real_LRDD/test/images\"\n","TEST_LBL_DIR = f\"{BASE_DATA_PATH}//real_LRDD/test/labels\"\n","\n","# Crea il dataset di test (riutilizzando la classe YoloTxtDetectionDataset)\n","test_dataset = YoloTxtDetectionDataset(TEST_IMG_DIR, TEST_LBL_DIR)\n","\n","test_loader = DataLoader(\n","    test_dataset, batch_size=4, shuffle=False, collate_fn=det_collate, num_workers=2\n",")\n","print(f\"Set di test caricato. Totale immagini: {len(test_dataset)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2OeJJDBjTBS","executionInfo":{"status":"ok","timestamp":1764283583955,"user_tz":-60,"elapsed":38,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"e44c741e-fd9d-4271-f22a-504cb35e1db6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Set di test caricato. Totale immagini: 1734\n"]}]},{"cell_type":"markdown","source":["**Evaluation on confidence and number of predicted boxes**"],"metadata":{"id":"ZhskSyrDjlo5"}},{"cell_type":"markdown","source":["Installation"],"metadata":{"id":"RL37MazV03b2"}},{"cell_type":"code","source":["# Cella 1: Installazione delle dipendenze corrette\n","\n","print(\"Installing compatible torch and torchvision versions...\")\n","# Installazione di torch 2.2.2 e torchvision 0.17.2, una combinazione stabile.\n","#!pip install torch==2.2.2 torchvision==0.17.2 --extra-index-url https://download.pytorch.org/whl/cu121\n","\n","# Installazione delle utility di valutazione (pycocotools)\n","#!pip install cython pycocotools\n","\n","# Clonazione e copia dei file ausiliari mancanti\n","#!git clone https://github.com/pytorch/vision.git\n","#!cp vision/references/detection/utils.py .\n","#!cp vision/references/detection/coco_utils.py .\n","\n","print(\"Installazione completa. SI PREGA DI RIAVVIARE IL RUNTIME ORA.\")"],"metadata":{"id":"RGqhFV-ujqcQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FIX: Downgrade NumPy to a compatible version\n","# NumPy 2.0.2 can cause RuntimeErrors with some PyTorch/Torchvision installations.\n","print(\"Uninstalling current NumPy and installing compatible version...\")\n","#!pip uninstall -y numpy\n","#!pip install numpy==1.26.4\n","print(\"NumPy installation complete. Please RESTART RUNTIME (Runtime -> Restart runtime) before proceeding.\")"],"metadata":{"id":"LAv35Ok-wwZ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes = 2 # Background (0) + Drone Class (1)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# DAFasterRCNN_Global_Stable needs to be re-defined or imported if runtime was restarted\n","# For this fix, I'll assume the model definition cell (IfD71-xMav5I) was re-executed.\n","\n","# Re-create model instance and load state dict\n","model = DAFasterRCNN_Global_Stable( # FIX: Corrected typo in class name\n","    num_classes=num_classes,\n","    lambda_da_max=1, # These parameters are needed for instantiation\n","    da_warmup_steps=1000 # but not used in inference mode\n",").to(device)\n","\n","CHECKPOINT_FILENAME = 'da_frcnn_epoch_29.pth'\n","DRIVE_SAVE_DIR = '/content/drive/MyDrive/Checkpoints'\n","FINAL_CHECKPOINT_PATH = os.path.join(DRIVE_SAVE_DIR, CHECKPOINT_FILENAME)\n","\n","print(f\"Caricamento pesi da: {FINAL_CHECKPOINT_PATH}\")\n","checkpoint = torch.load(FINAL_CHECKPOINT_PATH, map_location=device)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.eval()\n","\n","# Re-create test dataset and loader\n","TEST_IMG_DIR = '/content/data_extracted/real_LRDD/test/images'\n","TEST_LBL_DIR = '/content/data_extracted/real_LRDD/test/labels'\n","\n","\n","\n","# Re-crea dataset e loader\n","test_dataset = YoloTxtDetectionDataset(TEST_IMG_DIR, TEST_LBL_DIR)\n","\n","SUBSET_SIZE = 20 #\n","subset_indices = list(range(SUBSET_SIZE))\n","test_dataset_subset = torch.utils.data.Subset(test_dataset, subset_indices)\n","print(f\"Set di test limitato a {SUBSET_SIZE} immagini per la verifica gratuita.\")\n","# *******************************************************************\n","\n","# Usiamo il Batch Size 1 (o 2) e num_workers=0 per massima stabilità\n","test_loader = DataLoader(\n","    test_dataset_subset, batch_size=2, shuffle=False, num_workers=0, collate_fn=det_collate\n",")\n","\n","\n","\n","from tqdm.notebook import tqdm\n","import torch\n","import torch.nn.functional as F\n","\n","\n","# Calcoliamo la Confidence Media e il numero totale di box predetti.\n","\n","def run_simple_test(model, data_loader, device):\n","    model.eval()\n","    total_detections = 0\n","    total_confidence = 0.0\n","\n","    # Questo ciclo serve solo per verificare che il modello generi output validi\n","    for images, targets in tqdm(data_loader, desc=\"Esecuzione Inferenza Veloce\"):\n","        images = list(img.to(device) for img in images)\n","\n","        with torch.no_grad():\n","            outputs = model(images)\n","\n","        for output in outputs:\n","            # outputs è una lista di dizionari, uno per immagine\n","            scores = output['scores']\n","\n","            if len(scores) > 0:\n","                total_detections += len(scores)\n","                total_confidence += scores.mean().item()\n","\n","    if total_detections == 0:\n","        print(\"\\n[RISULTATO V.0] NESSUN DRONE RILEVATO. Il modello è probabilmente inefficiente o ha threshold troppo alti.\")\n","        return\n","\n","    avg_confidence = total_confidence / total_detections\n","\n","    print(\"\\n--- RISULTATO TEST VELOCE (Solo Sanità) ---\")\n","    print(f\"Immagini Processate: {len(data_loader.dataset)}\")\n","    print(f\"Totale Bounding Box Predette: {total_detections}\")\n","    print(f\"Confidenza Media delle Predizioni: {avg_confidence:.4f}\")\n","\n","    if avg_confidence > 0.7:\n","        print(\"✅ SUCCESS: Il modello genera output ad alta confidenza. Vale la pena acquistare i crediti Premium per la valutazione mAP completa.\")\n","    else:\n","        print(\"⚠️ AVVISO: La confidenza è bassa. Il modello potrebbe non aver imparato bene o ha bisogno di più epoche.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6mAxBsaI6SbF","executionInfo":{"status":"ok","timestamp":1764320752363,"user_tz":-60,"elapsed":1575,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"50426176-01de-4e7d-9214-1f8578fb7561"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Caricamento pesi da: /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_29.pth\n","Set di test limitato a 20 immagini per la verifica gratuita.\n"]}]},{"cell_type":"code","source":["run_simple_test(model, test_loader, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":153,"referenced_widgets":["189ea337ea4e488088bc36b5535dccfe","fcde33f0157742b3bb145a11047b4cd1","61a0f4c028274845872bb2c67f388f83","f80b711abe644ebfa1cef82191e263b0","e53c373301ba4a72a9d34692b8909ceb","28afef4df1494d7780272115b8425760","32694a5b753a46db88ace75fced4de60","d8336df6bfcb457898873b6d9467481a","7bf8e3f52cb5429ebfdde1d50cfef0db","6eca9acfd5a94e42a8b82e2e8ed9dfe5","bcb590872e904198b9a2d4c03ee53a99"]},"id":"Ad2eFXUz6VXL","executionInfo":{"status":"ok","timestamp":1764320760461,"user_tz":-60,"elapsed":4203,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"0d4ba112-cb2e-40a9-b3f1-ca2e6486af9c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Esecuzione Inferenza Veloce:   0%|          | 0/10 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"189ea337ea4e488088bc36b5535dccfe"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","--- RISULTATO TEST VELOCE (Solo Sanità) ---\n","Immagini Processate: 20\n","Totale Bounding Box Predette: 2000\n","Confidenza Media delle Predizioni: 0.0053\n","⚠️ AVVISO: La confidenza è bassa. Il modello potrebbe non aver imparato bene o ha bisogno di più epoche.\n"]}]},{"cell_type":"markdown","source":["**t-SNE**"],"metadata":{"id":"G5uAkuzjchfm"}},{"cell_type":"code","source":["/content/data_extracted/virtualonlymountains\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764772151838,"user_tz":-60,"elapsed":275685,"user":{"displayName":"Giorgio Filippo Caretti","userId":"01676580551476783131"}},"outputId":"c3a25603-87e3-451e-d4c2-0898ebb72e4a","id":"EMckpkb9nxmt"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","2. Copia dati virtuali aggiuntivi (Montagna) in corso...\n","Copia completata. Dati montagna disponibili in: /content/data_extracted/virtualonlymountains\n","\n","Tutte le operazioni sui dati sono complete.\n"]}]},{"cell_type":"markdown","source":["t-SNE before city images, change with mixed dataframe when willing to see the difference"],"metadata":{"id":"plsolnYJoQkg"}},{"cell_type":"code","source":["import torch\n","import tqdm\n","import torch.nn.functional as F\n","from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms.functional as TF\n","from PIL import Image\n","import os\n","from glob import glob\n","from collections import OrderedDict\n","\n","# Librerie di analisi e plot (Devono essere installate in Colab: !pip install scikit-learn matplotlib)\n","from sklearn.manifold import TSNE\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm # FIX: Import tqdm function directly from tqdm.notebook\n","\n","# --- 1. CLASSE DATASET SEMPLIFICATA PER L'ANALISI ---\n","\n","class SimpleImageDataset(Dataset):\n","    \"\"\"Dataset semplificato che carica solo le immagini per l'analisi delle feature.\"\"\"\n","    def __init__(self, img_dir: str):\n","        self.img_paths = sorted(\n","            [p for p in glob(os.path.join(img_dir, \"*\")) if p.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n","        )\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        # Le feature vengono estratte dal Backbone di Faster R-CNN, che si aspetta\n","        # il formato [C, H, W] in [0, 1].\n","        img_tensor = TF.to_tensor(img)\n","        return img_tensor, img_path # Restituiamo il percorso per il debug\n","\n","# --- 2. FUNZIONE DI ESTRAZIONE FEATURE ---\n","\n","def extract_features(model_backbone, data_loader, device):\n","    \"\"\"Estrae le feature globali (pooling FPN) per tutte le immagini.\"\"\"\n","    all_features = []\n","\n","    # Metti il modello in modalità valutazione\n","    model_backbone.eval()\n","\n","    with torch.no_grad():\n","        for images, paths in tqdm(data_loader, desc=\"Extracting Features\"):\n","            images = [img.to(device) for img in images]\n","\n","            # Forward pass solo attraverso il backbone\n","            features = model_backbone(images[0].unsqueeze(0)) # Passa un'immagine alla volta\n","\n","            if isinstance(features, torch.Tensor):\n","                features = OrderedDict([(\"0\", features)])\n","\n","            # Global pooled features (concatena i feature di tutti i livelli FPN)\n","            pooled = [\n","                F.adaptive_avg_pool2d(f, 1).flatten(1)\n","                for f in features.values()\n","            ]\n","            im_feat = torch.cat(pooled, dim=1)\n","\n","            all_features.append(im_feat.cpu().numpy())\n","\n","    return np.concatenate(all_features, axis=0)\n","\n","# --- 3. ESECUZIONE E VISUALIZZAZIONE PRINCIPALE ---\n","\n","if __name__ == '__main__':\n","    # CONFIGURAZIONE (Adatta questi percorsi e parametri)\n","    BASE_DATA_PATH = '/content/data_extracted'\n","    REAL_IMG_DIR = os.path.join(BASE_DATA_PATH, 'real_LRDD/train/images') # Target\n","    VIRT_IMG_DIR = os.path.join(BASE_DATA_PATH, 'virtualonlymountains')  # Sorgente\n","\n","    # Parametri\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    SAMPLE_SIZE = 1000 # Numero massimo di campioni da campionare per il plot T-SNE\n","\n","    # --- 3.1. Preparazione del Backbone (Usiamo i pesi COCO) ---\n","    weights = FasterRCNN_ResNet50_FPN_Weights.COCO_V1\n","    base_model = fasterrcnn_resnet50_fpn(weights=weights)\n","    backbone = base_model.backbone.to(device)\n","\n","    # --- 3.2. Caricamento Dati ---\n","    virtual_dataset = SimpleImageDataset(VIRT_IMG_DIR)\n","    real_dataset = SimpleImageDataset(REAL_IMG_DIR)\n","\n","    # Veloce campionamento dei dati per T-SNE (riduce il tempo di esecuzione)\n","    if len(virtual_dataset) > SAMPLE_SIZE:\n","        virtual_indices = np.random.choice(len(virtual_dataset), SAMPLE_SIZE, replace=False)\n","        virtual_subset = torch.utils.data.Subset(virtual_dataset, virtual_indices)\n","    else:\n","        virtual_subset = virtual_dataset\n","\n","    if len(real_dataset) > SAMPLE_SIZE:\n","        real_indices = np.random.choice(len(real_dataset), SAMPLE_SIZE, replace=False)\n","        real_subset = torch.utils.data.Subset(real_dataset, real_indices)\n","    else:\n","        real_subset = real_dataset\n","\n","    virtual_loader = DataLoader(virtual_subset, batch_size=1, shuffle=False)\n","    real_loader = DataLoader(real_subset, batch_size=1, shuffle=False)\n","\n","    # --- 3.3. Estrazione delle Feature ---\n","    print(\"Inizio estrazione feature Sorgente...\")\n","    virt_features = extract_features(backbone, virtual_loader, device)\n","\n","    print(\"Inizio estrazione feature Target...\")\n","    real_features = extract_features(backbone, real_loader, device)\n","\n","    # Combina feature e crea etichette di dominio (0=Virtual, 1=Reale)\n","    all_features = np.vstack([virt_features, real_features])\n","    domain_labels = np.array([0] * len(virt_features) + [1] * len(real_features))\n","\n","    # --- 3.4. Riduzione della Dimensionalità con T-SNE ---\n","    print(f\"Esecuzione T-SNE su {len(all_features)} campioni...\")\n","    # T-SNE è computazionalmente costoso, si usa PCA per una riduzione iniziale\n","    # se la dimensione dei dati è troppo alta, ma in questo caso usiamo l'implementazione base.\n","\n","    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000, learning_rate=200)\n","    tsne_results = tsne.fit_transform(all_features)\n","\n","    # --- 3.5. Visualizzazione ---\n","    plt.figure(figsize=(10, 8))\n","\n","    # Plot Dominio Sorgente (Virtuale)\n","    plt.scatter(\n","        tsne_results[domain_labels == 0, 0],\n","        tsne_results[domain_labels == 0, 1],\n","        label='Sorgente (Virtuale)',\n","        alpha=0.5,\n","        color='blue'\n","    )\n","\n","    # Plot Dominio Target (Reale)\n","    plt.scatter(\n","        tsne_results[domain_labels == 1, 0],\n","        tsne_results[domain_labels == 1, 1],\n","        label='Target (Reale)',\n","        alpha=0.5,\n","        color='red'\n","    )\n","\n","    plt.title(\"Analisi T-SNE delle Feature del Backbone Pre-Addestrato\")\n","    plt.xlabel(\"Componente T-SNE 1\")\n","    plt.ylabel(\"Componente T-SNE 2\")\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","\n","    # Salva e mostra il plot (Colab visualizza plt.show() automaticamente)\n","    plt.savefig('tsne_domain_shift.png')\n","    plt.show()\n","    print(\"Analisi T-SNE completata. Controlla il plot 'tsne_domain_shift.png'.\")"],"metadata":{"id":"5pcOjVgFcg6Z"},"execution_count":null,"outputs":[]}]}