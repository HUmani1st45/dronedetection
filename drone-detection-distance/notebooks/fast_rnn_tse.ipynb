{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definition, inspired by Domain Adaptive Faster R-CNN for Object Detection in the Wild\" (Chen et al., 2018)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMzwNMYMy1mK"
   },
   "source": [
    "Only mountains virtual images are loaded in a drive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292459,
     "status": "ok",
     "timestamp": 1764775388608,
     "user": {
      "displayName": "Giorgio Filippo Caretti",
      "userId": "01676580551476783131"
     },
     "user_tz": -60
    },
    "id": "u8X8HSX3hk3B",
    "outputId": "489039c9-0607-4f69-d9c5-73df564a0dd9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# we transer them in the virtual env\n",
    "SOURCE_IMAGES_DIR= '/content/drive/MyDrive/virtualonlymountains'\n",
    "UNZIP_PATH = '/content/data_extracted'\n",
    "\n",
    "\n",
    "DEST_IMAGES_DIR = os.path.join(UNZIP_PATH, 'virtualonlymountains')\n",
    "\n",
    "\n",
    "if os.path.exists(SOURCE_IMAGES_DIR):\n",
    "    print(\"\\n2. Copia dati virtuali aggiuntivi (Montagna) \")\n",
    "\n",
    "    # taregt folder must exist\n",
    "    os.makedirs(DEST_IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "    # copying data to dest folder\n",
    "    !cp -R \"{SOURCE_IMAGES_DIR}/\"* \"{DEST_IMAGES_DIR}\"\n",
    "\n",
    "\n",
    "    print(f\"Copia completata. Dati montagna disponibili in: {DEST_IMAGES_DIR}\")\n",
    "else:\n",
    "    print(f\"\\nAVVISO: Cartella virtualonlymountains non trovata in {SOURCE_IMAGES_DIR}\")\n",
    "\n",
    "print(\"\\nTutte le operazioni sui dati sono complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1764775410894,
     "user": {
      "displayName": "Giorgio Filippo Caretti",
      "userId": "01676580551476783131"
     },
     "user_tz": -60
    },
    "id": "6o7UZXP4x1E5",
    "outputId": "4bb3fcfd-a7a2-4ef4-bc90-b74c4c11a140"
   },
   "outputs": [],
   "source": [
    "# Verifying all images are loaded\n",
    "\n",
    "DEST_IMAGES_DIR = '/content/data_extracted/virtualonlymountains'\n",
    "\n",
    "print(f\"Ispezione della directory: {DEST_IMAGES_DIR}\")\n",
    "\n",
    "# Elenca i primi 10 file per confermare che i dati siano presenti\n",
    "print(\"\\nPrimi 10 file copiati:\")\n",
    "!ls {DEST_IMAGES_DIR} | head -n 10\n",
    "\n",
    "# Conta il numero totale di file (per assicurarsi che siano tutti stati copiati)\n",
    "print(\"\\nConteggio totale delle immagini:\")\n",
    "!ls {DEST_IMAGES_DIR} | wc -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnKtU4WMyw6g"
   },
   "source": [
    "Zip file with train real and virtual ( only mountains ) is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2253,
     "status": "ok",
     "timestamp": 1764775449847,
     "user": {
      "displayName": "Giorgio Filippo Caretti",
      "userId": "01676580551476783131"
     },
     "user_tz": -60
    },
    "id": "rPpKkdrXV3CF",
    "outputId": "f65bf5a2-2e89-4a83-b91a-1f9df0eaa09b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# 1. Monta Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Definisci il percorso del tuo file ZIP su Drive\n",
    "# DEVI SOSTITUIRE 'My Drive' e 'Drone_Project_Data' con i nomi esatti delle tue cartelle\n",
    "ZIP_PATH_ON_DRIVE = '/content/drive/MyDrive/zip_data_cv.zip'\n",
    "\n",
    "# Definisci la directory dove i dati verranno decompressi (nella memoria temporanea di Colab)\n",
    "UNZIP_PATH = '/content/data_extracted'\n",
    "\n",
    "if not os.path.exists(UNZIP_PATH):\n",
    "    os.makedirs(UNZIP_PATH)\n",
    "\n",
    "print(f\"Percorso del file ZIP su Drive: {ZIP_PATH_ON_DRIVE}\")\n",
    "print(f\"Percorso di destinazione per la decompressione: {UNZIP_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329171,
     "status": "ok",
     "timestamp": 1764776219725,
     "user": {
      "displayName": "Giorgio Filippo Caretti",
      "userId": "01676580551476783131"
     },
     "user_tz": -60
    },
    "id": "o1YUOsM3W2uj",
    "outputId": "83924dca-7854-40f5-ffbb-64d6258d2aa4"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Decompression\n",
    "\n",
    "print(\"wait...\")\n",
    "\n",
    "# RCleaning old folders\n",
    "if os.path.exists(os.path.join(UNZIP_PATH, 'real_LRDD')):\n",
    "    shutil.rmtree(os.path.join(UNZIP_PATH, 'real_LRDD'))\n",
    "\n",
    "# Execute\n",
    "!unzip -q -o {ZIP_PATH_ON_DRIVE} -d {UNZIP_PATH}\n",
    "\n",
    "print(\"completed\")\n",
    "# Verify\n",
    "!ls {UNZIP_PATH}/real_LRDD/train/images | head -n 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST6FFD2IajSJ"
   },
   "source": [
    "Verifying new path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1764775493204,
     "user": {
      "displayName": "Giorgio Filippo Caretti",
      "userId": "01676580551476783131"
     },
     "user_tz": -60
    },
    "id": "7-LuTkcOZNXK",
    "outputId": "c636cb58-6330-4cf3-e1e4-c81d1b8b325b"
   },
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = UNZIP_PATH\n",
    "\n",
    "virt_img_dir = f\"{BASE_DATA_PATH}/virtual/train/images\"\n",
    "virt_lbl_dir = f\"{BASE_DATA_PATH}/virtual/train/labels\"\n",
    "real_img_dir = f\"{BASE_DATA_PATH}/real_LRDD/train/images\"\n",
    "real_lbl_dir = f\"{BASE_DATA_PATH}/real_LRDD/train/labels\"\n",
    "\n",
    "\n",
    "# Verify paths (sanity check)\n",
    "print(f\"Verified Source Images: {virt_img_dir}\")\n",
    "print(f\"Verified Target Images: {real_img_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPffYx9Jpp15"
   },
   "source": [
    "Verifying the presence of all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyJssS9A0z14"
   },
   "outputs": [],
   "source": [
    "def check_dataset_presence(base_path):\n",
    "    \"\"\"Controlla se le cartelle chiave esistono e quanti file contengono.\"\"\"\n",
    "\n",
    "    # Paths \n",
    "    paths_to_check = {\n",
    "        \"Source (Virtual) Images\": os.path.join(base_path, 'virtual/train/images'),\n",
    "        \"Source (Virtual) Labels\": os.path.join(base_path, 'virtual/train/labels'),\n",
    "        \"Target (Real) Images\": os.path.join(base_path, 'real_LRDD/train/images'),\n",
    "        \"Target (Real) Labels\": os.path.join(base_path, 'real_LRDD/train/labels'),\n",
    "        \"Virtual Mountains Images (Added)\": os.path.join(base_path, 'virtualmountains/train/images'),\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- RESULTS ---\")\n",
    "\n",
    "    all_ok = True\n",
    "    for name, path in paths_to_check.items():\n",
    "        if os.path.isdir(path):\n",
    "            count = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "            print(f\" {name}: Found ({count} file)\")\n",
    "        else:\n",
    "            print(f\"{name}: Not found (missing path: {path})\")\n",
    "            all_ok = False\n",
    "\n",
    "    if all_ok:\n",
    "        print(\"\\nSETUP CONFIRMED\")\n",
    "    else:\n",
    "        print(\"\\nSETUP NOT CONFIRMED\")\n",
    "\n",
    "# ESECUZIONE DELLA VERIFICA\n",
    "# In Colab, le variabili definite nelle celle precedenti sono globali.\n",
    "check_dataset_presence(UNZIP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-_4K8g0p4rL"
   },
   "source": [
    "**Function set up**\n",
    "Installing things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p__omqpiaq4J"
   },
   "outputs": [],
   "source": [
    "# installing torchvision\n",
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFCXW9e8bN15"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from typing import List, Dict, Tuple\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils as utils # For gradient clipping\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn,\n",
    "    FasterRCNN_ResNet50_FPN_Weights\n",
    ")\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfD71-xMav5I"
   },
   "outputs": [],
   "source": [
    "# Creating the gradient reverse function\n",
    "class GradientReverseFn(autograd.Function):\n",
    "    \"\"\"Handles the forward (identity) and backward (gradient reversal) pass.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        # Save the dynamic lambda for the backward pass. This parameter\n",
    "        # controls the strength of the domain adaptation loss.\n",
    "        ctx.lambda_ = lambda_\n",
    "        # Forward pass is just an identity function: output = input\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        #  Reverse and scale the gradient.\n",
    "        # This makes the feature extractor (backbone) learn domain-invariant features,\n",
    "        # as it tries to *maximize* the domain classification loss.\n",
    "        return -ctx.lambda_ * grad_output, None # The 'None' handles the gradient for lambda_\n",
    "\n",
    "#A standard PyTorch Module wrapper for the GRL Function.\n",
    "# This makes it easy to integrate into the model's forward method.\n",
    "class GradientReverse(nn.Module):\n",
    "    \"\"\"Module wrapper for GRL, accepts lambda in forward for dynamic scheduling.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, lambda_):\n",
    "        # Pass the dynamically calculated lambda_ directly to the function\n",
    "        return GradientReverseFn.apply(x, lambda_)\n",
    "\n",
    "# ==============================================================================\n",
    "# DA-FASTER R-CNN MODEL FOR GLOBAL IMAGE-LEVEL DOMAIN ADAPTATION\n",
    "# Architecture inspired by: \"Domain Adaptive Faster R-CNN for Object Detection in the Wild\" (Chen et al., 2018)\n",
    "\n",
    "class DAFasterRCNN_Global_Stable(nn.Module):\n",
    "    \"\"\"\n",
    "    Domain-Adaptive Faster R-CNN with COCO pre-trained weights initialization\n",
    "    for a stable Task Loss. This version includes image-level domain adaptation.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, lambda_da_max=1.0, da_warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        # Hyperparameters for the Domain Adaptation (DA) component\n",
    "        self.lambda_da_max = lambda_da_max\n",
    "        self.da_warmup_steps = da_warmup_steps\n",
    "        self.current_step = 0 # To track training progress for the warm-up schedule\n",
    "\n",
    "        # --- 1. BASE DETECTOR CONFIGURATION (Faster R-CNN) ---\n",
    "\n",
    "        # Load COCO weights to pre-train the feature extractor (backbone)\n",
    "        weights = FasterRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "\n",
    "        # Custom anchors for drone detection (or small objects)\n",
    "        # Using smaller sizes is crucial for detecting objects like drones.\n",
    "        anchor_generator = AnchorGenerator(\n",
    "            sizes=((8, 16, 32), (16, 32, 64), (32, 64, 128), (64, 128, 256), (128, 256, 512)),\n",
    "            aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    "        )\n",
    "\n",
    "        # MANUAL BACKBONE WEIGHT LOADING TO AVOID RPN/BOX HEAD SIZE MISMATCH ---\n",
    "\n",
    "        # 1. Initialize the detector with the correct configuration (custom anchors, N classes)\n",
    "        # We start without weights because the RPN/Box Head dimensions are different from COCO's\n",
    "        self.detector = fasterrcnn_resnet50_fpn(\n",
    "            weights=None,\n",
    "            num_classes=num_classes, # E.g., background + drone (2 classes)\n",
    "            rpn_anchor_generator=anchor_generator\n",
    "        )\n",
    "\n",
    "        # 2. Create a temporary model only to fetch the pre-trained COCO weights\n",
    "        coco_model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "        # 3. Copy ONLY the backbone's weights from the COCO model to our model.\n",
    "        # This keeps the feature extraction strong while allowing the RPN and Box Heads\n",
    "        # to correctly initialize for our custom anchor sizes and number of classes.\n",
    "        self.detector.backbone.load_state_dict(coco_model.backbone.state_dict())\n",
    "\n",
    "        # The final layers (RPN Head and Box Predictor) maintain their default initialization\n",
    "        # (usually random) and the correct dimensions.\n",
    "\n",
    "        # --- 2. DOMAIN ADAPTATION (DA) CONFIGURATION ---\n",
    "        self.grl = GradientReverse()\n",
    "        self.im_domain_head = None # Will be lazily initialized based on feature size\n",
    "\n",
    "    def set_current_step(self, step):\n",
    "        \"\"\"Allows external update of the current iteration/step count.\"\"\"\n",
    "        self.current_step = step\n",
    "\n",
    "    def warmup_lambda(self):\n",
    "        \"\"\"Gradually increase lambda_da over first steps (linear schedule).\"\"\"\n",
    "        # A common technique to stabilize early-stage training: the DA loss\n",
    "        # is gradually introduced to allow the detector to learn basic features first.\n",
    "        if self.current_step >= self.da_warmup_steps:\n",
    "            return self.lambda_da_max\n",
    "\n",
    "        progress = self.current_step / self.da_warmup_steps\n",
    "        return progress * self.lambda_da_max\n",
    "\n",
    "    def _build_domain_head(self, feature_dict):\n",
    "        \"\"\"Initializes the domain classifier based on the feature dimensions.\"\"\"\n",
    "        num_scales = len(feature_dict) # Number of FPN levels (usually 5)\n",
    "        # Assuming FPN gives 256-dim features (p2, p3, p4, p5, p6)\n",
    "        feat_dim = next(iter(feature_dict.values())).shape[1]\n",
    "        input_dim = num_scales * feat_dim # Total size after concatenation\n",
    "\n",
    "        # Domain Classifier Head: simple MLP for binary classification (Source vs. Target)\n",
    "        self.im_domain_head = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2) # Output: Logits for Source (0) and Target (1)\n",
    "        ).to(next(self.parameters()).device)\n",
    "\n",
    "    def forward(self, images, targets=None, domain=None):\n",
    "        if self.training and domain is None:\n",
    "            raise ValueError(\"Need domain='source' or 'target' during training.\")\n",
    "\n",
    "        # Inference mode: standard Faster R-CNN forward pass\n",
    "        if not self.training:\n",
    "            self.detector.eval()\n",
    "            with torch.no_grad():\n",
    "                return self.detector(images)\n",
    "\n",
    "        # --- TRAINING MODE ---\n",
    "\n",
    "        # 1. Feature Extraction (Backbone + FPN)\n",
    "        images_t, targets_t = self.detector.transform(images, targets)\n",
    "        features = self.detector.backbone(images_t.tensors)\n",
    "        # Ensure features are a dictionary for FPN (handle single-tensor case)\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = OrderedDict([(\"0\", features)])\n",
    "\n",
    "        # 2. Lazy initialization: Build Domain Head if it hasn't been done yet\n",
    "        if self.im_domain_head is None:\n",
    "            self._build_domain_head(features)\n",
    "\n",
    "        det_losses = {}\n",
    "        # 3. DETECTION TASK LOSS (Only calculated on labeled SOURCE data)\n",
    "        if domain == \"source\":\n",
    "            # Call the standard Faster R-CNN forward to get RPN and Box losses\n",
    "            det_outputs = self.detector.forward(images, targets)\n",
    "            det_losses.update(det_outputs)\n",
    "\n",
    "        # 4. DOMAIN ADAPTATION LOSS (Applied to both Source and Target)\n",
    "        da_losses = {}\n",
    "\n",
    "        # Domain label (0=source, 1=target) for the Cross-Entropy loss\n",
    "        dom_label = 0 if domain == \"source\" else 1\n",
    "        dom_label_tensor = torch.full(\n",
    "            (len(images),), dom_label, dtype=torch.long, device=images[0].device\n",
    "        )\n",
    "\n",
    "        # Global pooled features: Aggregate features from all FPN levels (P2-P5)\n",
    "        pooled = [\n",
    "            # Global Average Pooling (GAP) on each feature map to get a fixed-size vector\n",
    "            F.adaptive_avg_pool2d(f, 1).flatten(1)\n",
    "            for f in features.values()\n",
    "        ]\n",
    "        im_feat = torch.cat(pooled, dim=1) # Concatenate the features: [Batch, Total_Feature_Dim]\n",
    "\n",
    "        # 5. GRL with dynamic warm-up\n",
    "        lambda_da = self.warmup_lambda()\n",
    "        # Apply GRL: gradient is reversed and scaled by lambda_da.\n",
    "        # This makes the backbone learn domain-invariant features.\n",
    "        rev_feat = self.grl(im_feat, lambda_da)\n",
    "\n",
    "        # Classify the domain based on the reversed features\n",
    "        logits = self.im_domain_head(rev_feat)\n",
    "        # Calculate DA loss (Cross-Entropy)\n",
    "        da_losses[\"loss_da_im\"] = F.cross_entropy(logits, dom_label_tensor)\n",
    "\n",
    "        # 6. Combine all losses (Detection Loss + Domain Adaptation Loss)\n",
    "        losses = {}\n",
    "        losses.update(det_losses)\n",
    "        losses.update(da_losses)\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLm7MktUblch"
   },
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3VSBroHbkm4"
   },
   "outputs": [],
   "source": [
    "class YoloTxtDetectionDataset(Dataset):\n",
    "    \"\"\"Loads images and converts YOLO format labels (normalized cx,cy,w,h)\n",
    "    to TorchVision format (absolute x1,y1,x2,y2).\"\"\"\n",
    "    def __init__(self, img_dir: str, label_dir: str, transforms=None):\n",
    "        self.img_paths = sorted(\n",
    "            [p for p in glob(os.path.join(img_dir, \"*\")) if p.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
    "        )\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def _load_yolo_labels(self, img_path: str, w: int, h: int):\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        label_path = os.path.join(self.label_dir, base + \".txt\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        if not os.path.exists(label_path):\n",
    "            # Crucial: return empty tensors if no labels found\n",
    "            return torch.empty((0, 4), dtype=torch.float32), torch.empty((0,), dtype=torch.int64)\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                parts = line.split()\n",
    "                # YOLO format: class_id cx cy w h (normalized to 0-1)\n",
    "                cls = int(parts[0])\n",
    "                x_c, y_c, bw, bh = map(float, parts[1:5])\n",
    "\n",
    "                # Convert normalized cx, cy, w, h → absolute pixel xyxy\n",
    "                x_c, y_c, bw, bh = x_c * w, y_c * h, bw * w, bh * h\n",
    "                x1 = x_c - bw / 2\n",
    "                y1 = y_c - bh / 2\n",
    "                x2 = x_c + bw / 2\n",
    "                y2 = y_c + bh / 2\n",
    "\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                # IMPORTANT: Faster R-CNN reserves class ID 0 for background.\n",
    "                # If YOLO is 0-indexed, shift classes by 1.\n",
    "                labels.append(cls + 1)\n",
    "\n",
    "        if boxes:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.empty((0, 4), dtype=torch.float32)\n",
    "            labels = torch.empty((0,), dtype=torch.int64)\n",
    "\n",
    "        return boxes, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        boxes, labels = self._load_yolo_labels(img_path, w, h)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            # Required for TorchVision: area and iscrowd\n",
    "            \"area\": (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) if boxes.numel() > 0 else torch.tensor([], dtype=torch.float32),\n",
    "            \"iscrowd\": torch.zeros((boxes.shape[0],), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        # Convert PIL Image to Tensor\n",
    "        img = TF.to_tensor(img)\n",
    "        return img, target\n",
    "\n",
    "def det_collate(batch):\n",
    "    \"\"\"Custom collate function required by TorchVision detection models.\"\"\"\n",
    "    imgs, targets = zip(*batch)\n",
    "    return list(imgs), list(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J_l3Cqnbxcp"
   },
   "source": [
    "Function of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYMoTQcObZFp"
   },
   "outputs": [],
   "source": [
    "def train_da_stable(\n",
    "    model,\n",
    "    source_loader,\n",
    "    target_loader,\n",
    "    optimizer,\n",
    "    num_epochs=15,\n",
    "    da_weight=0.001,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    model.train()\n",
    "    # Cycle the target loader so it runs as long as the source loader\n",
    "    target_iter = itertools.cycle(target_loader)\n",
    "    global_step = 0\n",
    "\n",
    "    # Initialize total_loss outside the loop for safe use in checkpointing on epoch 0\n",
    "    total_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    print(f\"Starting training on device: {device}\")\n",
    "    print(f\"Total batches per epoch: {len(source_loader)}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # 1. BATCH LOOP (INNER LOOP)\n",
    "        # -----------------------------------------------\n",
    "        for i, (src_imgs, src_tgts) in enumerate(source_loader):\n",
    "\n",
    "            # Update step and lambda schedule\n",
    "            global_step += 1\n",
    "            model.set_current_step(global_step)\n",
    "\n",
    "            src_imgs = [img.to(device) for img in src_imgs]\n",
    "            src_tgts = [{k: v.to(device) for k, v in t.items()} for t in src_tgts]\n",
    "\n",
    "            # SOURCE FORWARD PASS (Task Loss + DA Loss)\n",
    "            src_loss_dict = model(src_imgs, src_tgts, domain=\"source\")\n",
    "\n",
    "            # Aggregate detection task losses\n",
    "            det_loss = (\n",
    "                src_loss_dict.get(\"loss_classifier\", torch.tensor(0.0, device=device)) +\n",
    "                src_loss_dict.get(\"loss_box_reg\", torch.tensor(0.0, device=device)) +\n",
    "                src_loss_dict.get(\"loss_objectness\", torch.tensor(0.0, device=device)) +\n",
    "                src_loss_dict.get(\"loss_rpn_box_reg\", torch.tensor(0.0, device=device))\n",
    "            )\n",
    "            da_src = src_loss_dict.get(\"loss_da_im\", torch.tensor(0.0, device=device))\n",
    "\n",
    "            #  TARGET FORWARD PASS (DA Loss ONLY)\n",
    "            tgt_imgs, _ = next(target_iter)\n",
    "            tgt_imgs = [img.to(device) for img in tgt_imgs]\n",
    "\n",
    "            tgt_loss_dict = model(tgt_imgs, targets=None, domain=\"target\")\n",
    "            da_tgt = tgt_loss_dict.get(\"loss_da_im\", torch.tensor(0.0, device=device))\n",
    "\n",
    "            #  TOTAL LOSS & OPTIMIZATION\n",
    "            # Total Loss = Task Loss + DA Weight * (DA Source Loss + DA Target Loss)\n",
    "            total_loss = det_loss + da_weight * (da_src + da_tgt)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Gradient clipping for adversarial stability (CRITICAL)\n",
    "            utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # LOGGING\n",
    "            if global_step % 10 == 0:\n",
    "                current_lambda = model.warmup_lambda()\n",
    "                print(\n",
    "                    f\"[Epoch {epoch} Step {global_step}] \"\n",
    "                    f\"det={det_loss.item():.4f} \"\n",
    "                    f\"DA_src={da_src.item():.4f} \"\n",
    "                    f\"DA_tgt={da_tgt.item():.4f} \"\n",
    "                    f\"lambda={current_lambda:.6f} \"\n",
    "                    f\"total={total_loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # 2. CHECKPOINT SAVING (Runs once per epoch)\n",
    "        # -----------------------------------------------\n",
    "        save_path = f\"da_frcnn_epoch_{epoch}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'global_step': global_step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': total_loss.item(), # Last calculated loss of this epoch\n",
    "        }, save_path)\n",
    "        print(f\"\\nModel checkpoint saved to {save_path}\\n\")\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Io5qFTcZeHb5"
   },
   "source": [
    "Running it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1831,
     "status": "ok",
     "timestamp": 1764318426860,
     "user": {
      "displayName": "Giorgio Filippo Caretti",
      "userId": "01676580551476783131"
     },
     "user_tz": -60
    },
    "id": "8b4-fA7zeIdo",
    "outputId": "b2963bbf-0c17-4cbe-d0dd-8daee7a7ad40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla T4\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 148MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils as utils # For gradient clipping\n",
    "\n",
    "# NOTE: The train_da_stable function must be defined in the cell above (Section 3/4)\n",
    "\n",
    "# --- HYPERPARAMETERS ---\n",
    "num_classes = 2          # Background (0) + Drone Class (1)\n",
    "lambda_da_max = 0.0\n",
    "da_warmup_steps = 2000\n",
    "da_weight = 0.00005 #decreased from 0.0001\n",
    "num_epochs = 30\n",
    "batch_size = 2           # Use this small size for stability/memory\n",
    "num_workers = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  DEVICE & SETUP\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "#  DATA LOADERS\n",
    "source_dataset = YoloTxtDetectionDataset(virt_img_dir, virt_lbl_dir)\n",
    "target_dataset = YoloTxtDetectionDataset(real_img_dir, real_lbl_dir)\n",
    "\n",
    "source_loader = DataLoader(\n",
    "    source_dataset, batch_size=batch_size, shuffle=True, collate_fn=det_collate, num_workers=num_workers\n",
    ")\n",
    "target_loader = DataLoader(\n",
    "    target_dataset, batch_size=batch_size, shuffle=True, collate_fn=det_collate, num_workers=num_workers\n",
    ")\n",
    "\n",
    "# MODEL & OPTIMIZER\n",
    "model = DAFasterRCNN_Global_Stable(\n",
    "    num_classes=num_classes,\n",
    "    lambda_da_max=lambda_da_max,\n",
    "    da_warmup_steps=da_warmup_steps\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "#  LAUNCH TRAINING\n",
    "\n",
    "# CRITICAL: We modify the training function here to save directly to Google Drive\n",
    "# NOTE: This assumes the full train_da_stable function is available from Section 4 of your original script.\n",
    "\n",
    "def train_da_stable(\n",
    "    model, source_loader, target_loader, optimizer, num_epochs=10, da_weight=0.001, device=\"cuda\"\n",
    "):\n",
    "    model.train()\n",
    "    target_iter = itertools.cycle(target_loader)\n",
    "    global_step = 0\n",
    "    total_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    # Define the save directory on Google Drive\n",
    "    DRIVE_SAVE_DIR = os.path.join(BASE_DRIVE_PATH, \"Checkpoints\")\n",
    "    os.makedirs(DRIVE_SAVE_DIR, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "    #  (Rest of the train_da_stable logic runs as defined in your final script) ...\n",
    "    #\n",
    "    # PASTE THE FULL train_da_stable FUNCTION LOGIC HERE\n",
    "    #  REMEMBER TO MODIFY THE CHECKPOINT SAVE PATH!\n",
    "\n",
    "    # Example modified saving logic:\n",
    "    for epoch in range(num_epochs):\n",
    "        # ... (INNER BATCH LOOP CODE) ...\n",
    "        for i, (src_imgs, src_tgts) in enumerate(source_loader):\n",
    "            # ... (all the forward/backward/optimization logic) ...\n",
    "\n",
    "            # --- LOGGING (Important for Colab, as Colab sessions can be monitored) ---\n",
    "            if global_step % 100 == 0: # Log less frequently for large jobs\n",
    "                # ... print logs ...\n",
    "                pass\n",
    "\n",
    "        # 2. CHECKPOINT SAVING (Saves directly to Google Drive)\n",
    "        save_path = os.path.join(DRIVE_SAVE_DIR, f\"da_frcnn_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'global_step': global_step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': total_loss.item(),\n",
    "        }, save_path)\n",
    "        print(f\"\\nModel checkpoint saved to Google Drive at {save_path}\\n\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "# Launch the training function (make sure the full function body is defined in this cell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw61QcnIs3-6"
   },
   "source": [
    "Cell to verify the loss has been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9136,
     "status": "ok",
     "timestamp": 1764696476690,
     "user": {
      "displayName": "Giorgio Filippo Caretti",
      "userId": "01676580551476783131"
     },
     "user_tz": -60
    },
    "id": "1b2ZETA_2KOm",
    "outputId": "4b561331-d406-46a6-bfe9-6d7129c0ee59"
   },
   "outputs": [],
   "source": [
    "\n",
    "# checkpoint path\n",
    "CHECKPOINT_PATH = '/content/drive/MyDrive/Checkpoints/da_frcnn_epoch_1.pth'\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "\n",
    "    saved_loss = checkpoint.get('loss', 'N/A')\n",
    "\n",
    "    print(\"---  Checkpoint Epoch 0 ---\")\n",
    "    print(f\"Epoch saved: {checkpoint.get('epoch')}\")\n",
    "    print(f\"Global Step saved: {checkpoint.get('global_step')}\")\n",
    "    print(f\"Total Loss (Last batch): {saved_loss:.4f}\")\n",
    "\n",
    "    if saved_loss > 1.0:\n",
    "        print(\"det loss still high\")\n",
    "    else:\n",
    "        print(\"det loss decreased\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR in loading {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1764318435322,
     "user": {
      "displayName": "Giorgio Filippo Caretti",
      "userId": "01676580551476783131"
     },
     "user_tz": -60
    },
    "id": "koXpXkSIluqp",
    "outputId": "e486313c-6770-437b-8883-53616b7ebc0a"
   },
   "outputs": [],
   "source": [
    "# Path Configuration\n",
    "\n",
    "BASE_DRIVE_PATH = '/content/drive/MyDrive/'\n",
    "\n",
    "# L'output (checkpoint) verrà salvato qui: /content/drive/MyDrive/Drone_Project_Data/Checkpoints\n",
    "DRIVE_SAVE_DIR = os.path.join(BASE_DRIVE_PATH, \"Checkpoints\")\n",
    "os.makedirs(DRIVE_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "if not os.path.isdir(virt_img_dir):\n",
    "    raise FileNotFoundError(f\"path not found {virt_img_dir}\")\n",
    "\n",
    "print(f\"checkpoints saved: {DRIVE_SAVE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1891889,
     "status": "ok",
     "timestamp": 1764320335358,
     "user": {
      "displayName": "Giorgio Filippo Caretti",
      "userId": "01676580551476783131"
     },
     "user_tz": -60
    },
    "id": "T0sQiFsZfBcJ",
    "outputId": "6b871ed8-61d0-474e-9f21-1c01546e6f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_0.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_1.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_2.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_3.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_4.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_5.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_6.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_7.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_8.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_9.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_10.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_11.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_12.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_13.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_14.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_15.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_16.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_17.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_18.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_19.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_20.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_21.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_22.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_23.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_24.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_25.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_26.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_27.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_28.pth\n",
      "\n",
      "\n",
      "Model checkpoint saved to Google Drive at /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_29.pth\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "train_da_stable(model, source_loader, target_loader, optimizer, num_epochs=num_epochs, da_weight=da_weight, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R76Kh9f-i134"
   },
   "source": [
    "Valuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2533,
     "status": "ok",
     "timestamp": 1764320739433,
     "user": {
      "displayName": "Giorgio Filippo Caretti",
      "userId": "01676580551476783131"
     },
     "user_tz": -60
    },
    "id": "5vdx1B5Ci3zP",
    "outputId": "85cd7c42-b545-405b-a577-8fa1138ec64d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento pesi da: /content/drive/MyDrive/Checkpoints/da_frcnn_epoch_29.pth\n",
      "Modello caricato e impostato in modalità di valutazione (model.eval()). Pronto per il test.\n"
     ]
    }
   ],
   "source": [
    "# Cell to have a lightweight evaluation of the model\n",
    "\n",
    "import torch\n",
    "\n",
    "# Import DAFasterRCNN_Global_Stable  \n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CHECKPOINT_FILENAME = 'da_frcnn_epoch_29.pth'\n",
    "DRIVE_SAVE_DIR = '/content/drive/MyDrive/Checkpoints' \n",
    "FINAL_CHECKPOINT_PATH = os.path.join(DRIVE_SAVE_DIR, CHECKPOINT_FILENAME)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 2 # Background + 1 class Drone\n",
    "\n",
    "# 1. Inizialize the model\n",
    "model = DAFasterRCNN_Global_Stable(\n",
    "    num_classes=num_classes,\n",
    "    \n",
    "    lambda_da_max=1.0,\n",
    "    da_warmup_steps=1000\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoints\n",
    "print(f\"Caricamento pesi da: {FINAL_CHECKPOINT_PATH}\")\n",
    "checkpoint = torch.load(FINAL_CHECKPOINT_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Modello caricato e impostato in modalità di valutazione (model.eval()). Pronto per il test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XWVgYfIjSDJ"
   },
   "source": [
    "Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1764283583955,
     "user": {
      "displayName": "Giorgio Filippo Caretti",
      "userId": "01676580551476783131"
     },
     "user_tz": -60
    },
    "id": "V2OeJJDBjTBS",
    "outputId": "e44c741e-fd9d-4271-f22a-504cb35e1db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set di test caricato. Totale immagini: 1734\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "TEST_IMG_DIR = f\"{BASE_DATA_PATH}//real_LRDD/test/images\"\n",
    "TEST_LBL_DIR = f\"{BASE_DATA_PATH}//real_LRDD/test/labels\"\n",
    "\n",
    "# Build test dataset ( YoloTxtDetectionDataset)\n",
    "test_dataset = YoloTxtDetectionDataset(TEST_IMG_DIR, TEST_LBL_DIR)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=4, shuffle=False, collate_fn=det_collate, num_workers=2\n",
    ")\n",
    "print(f\"test set loaded, len dataset: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhskSyrDjlo5"
   },
   "source": [
    "**Evaluation on confidence and number of predicted boxes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RL37MazV03b2"
   },
   "source": [
    "Installation of packages to run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGqhFV-ujqcQ"
   },
   "outputs": [],
   "source": [
    "# Cella 1: Installation\n",
    "\n",
    "print(\"Installing compatible torch and torchvision versions...\")\n",
    "# torch 2.2.2 e torchvision 0.17.2\n",
    "#!pip install torch==2.2.2 torchvision==0.17.2 --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# utility  (pycocotools)\n",
    "#!pip install cython pycocotools\n",
    "\n",
    "\n",
    "#!git clone https://github.com/pytorch/vision.git\n",
    "#!cp vision/references/detection/utils.py .\n",
    "#!cp vision/references/detection/coco_utils.py .\n",
    "\n",
    "print(\"Installation completed, reactivate runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAv35Ok-wwZ_"
   },
   "outputs": [],
   "source": [
    "# FIX: Downgrade NumPy to a compatible version\n",
    "# NumPy 2.0.2 can cause RuntimeErrors with some PyTorch/Torchvision installations.\n",
    "print(\"Uninstalling current NumPy and installing compatible version...\")\n",
    "#!pip uninstall -y numpy\n",
    "#!pip install numpy==1.26.4\n",
    "print(\"NumPy installation complete. Please RESTART RUNTIME (Runtime -> Restart runtime) before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1575,
     "status": "ok",
     "timestamp": 1764320752363,
     "user": {
      "displayName": "Giorgio Filippo Caretti",
      "userId": "01676580551476783131"
     },
     "user_tz": -60
    },
    "id": "6mAxBsaI6SbF",
    "outputId": "50426176-01de-4e7d-9214-1f8578fb7561"
   },
   "outputs": [],
   "source": [
    "num_classes = 2 # Background (0) + Drone Class (1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DAFasterRCNN_Global_Stable needs to be re-defined or imported if runtime was restarted\n",
    "# For this fix, I'll assume the model definition cell (IfD71-xMav5I) was re-executed.\n",
    "\n",
    "# Re-create model instance and load state dict\n",
    "model = DAFasterRCNN_Global_Stable( # FIX: Corrected typo in class name\n",
    "    num_classes=num_classes,\n",
    "    lambda_da_max=1, # These parameters are needed for instantiation\n",
    "    da_warmup_steps=1000 # but not used in inference mode\n",
    ").to(device)\n",
    "\n",
    "CHECKPOINT_FILENAME = 'da_frcnn_epoch_29.pth'\n",
    "DRIVE_SAVE_DIR = '/content/drive/MyDrive/Checkpoints'\n",
    "FINAL_CHECKPOINT_PATH = os.path.join(DRIVE_SAVE_DIR, CHECKPOINT_FILENAME)\n",
    "\n",
    "print(f\"Weights from: {FINAL_CHECKPOINT_PATH}\")\n",
    "checkpoint = torch.load(FINAL_CHECKPOINT_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Re-create test dataset and loader\n",
    "TEST_IMG_DIR = '/content/data_extracted/real_LRDD/test/images'\n",
    "TEST_LBL_DIR = '/content/data_extracted/real_LRDD/test/labels'\n",
    "\n",
    "\n",
    "\n",
    "# Re-create dataset e loader\n",
    "test_dataset = YoloTxtDetectionDataset(TEST_IMG_DIR, TEST_LBL_DIR)\n",
    "\n",
    "SUBSET_SIZE = 20 #\n",
    "subset_indices = list(range(SUBSET_SIZE))\n",
    "test_dataset_subset = torch.utils.data.Subset(test_dataset, subset_indices)\n",
    "print(f\"Set di test limitato a {SUBSET_SIZE} immagini per la verifica gratuita.\")\n",
    "# *******************************************************************\n",
    "\n",
    "# Batch Size 1 (or 2) and num_workers=0 \n",
    "test_loader = DataLoader(\n",
    "    test_dataset_subset, batch_size=2, shuffle=False, num_workers=0, collate_fn=det_collate\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Average Confidence and predicted boxes\n",
    "\n",
    "def run_simple_test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_detections = 0\n",
    "    total_confidence = 0.0\n",
    "\n",
    "    #\n",
    "    for images, targets in tqdm(data_loader, desc=\"Esecuzione Inferenza Veloce\"):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "        for output in outputs:\n",
    "            # outputs is a list of dictionaries\n",
    "            scores = output['scores']\n",
    "\n",
    "            if len(scores) > 0:\n",
    "                total_detections += len(scores)\n",
    "                total_confidence += scores.mean().item()\n",
    "\n",
    "    if total_detections == 0:\n",
    "        print(\"\\n[ResultV.0] No drones.\")\n",
    "        return\n",
    "\n",
    "    avg_confidence = total_confidence / total_detections\n",
    "\n",
    "    print(\"\\n--- FAST TEST ---\")\n",
    "    print(f\"Processed images: {len(data_loader.dataset)}\")\n",
    "    print(f\"Total Bounding Boxes: {total_detections}\")\n",
    "    print(f\"Average Confidence of predictions: {avg_confidence:.4f}\")\n",
    "\n",
    "    if avg_confidence > 0.7:\n",
    "        print(\"✅ SUCCESS: high confidence\")\n",
    "    else:\n",
    "        print(\"⚠️ FAILURE: low confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "189ea337ea4e488088bc36b5535dccfe",
      "fcde33f0157742b3bb145a11047b4cd1",
      "61a0f4c028274845872bb2c67f388f83",
      "f80b711abe644ebfa1cef82191e263b0",
      "e53c373301ba4a72a9d34692b8909ceb",
      "28afef4df1494d7780272115b8425760",
      "32694a5b753a46db88ace75fced4de60",
      "d8336df6bfcb457898873b6d9467481a",
      "7bf8e3f52cb5429ebfdde1d50cfef0db",
      "6eca9acfd5a94e42a8b82e2e8ed9dfe5",
      "bcb590872e904198b9a2d4c03ee53a99"
     ]
    },
    "executionInfo": {
     "elapsed": 4203,
     "status": "ok",
     "timestamp": 1764320760461,
     "user": {
      "displayName": "Giorgio Filippo Caretti",
      "userId": "01676580551476783131"
     },
     "user_tz": -60
    },
    "id": "Ad2eFXUz6VXL",
    "outputId": "0d4ba112-cb2e-40a9-b3f1-ca2e6486af9c"
   },
   "outputs": [],
   "source": [
    "run_simple_test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5uAkuzjchfm"
   },
   "source": [
    "**t-SNE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plsolnYJoQkg"
   },
   "source": [
    "t-SNE before city images, change with mixed dataframe when willing to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import os\n",
    "from glob import glob\n",
    "from collections import OrderedDict\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pcOjVgFcg6Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SimpleImageDataset(Dataset):\n",
    "    \"\"\"Simplified Dataset\"\"\"\n",
    "    def __init__(self, img_dir: str):\n",
    "        self.img_paths = sorted(\n",
    "            [p for p in glob(os.path.join(img_dir, \"*\")) if p.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        \n",
    "        # format [C, H, W] in [0, 1].\n",
    "        img_tensor = TF.to_tensor(img)\n",
    "        return img_tensor, img_path \n",
    "\n",
    "# --- 2. EXTRACTION OF FEATURES\n",
    "\n",
    "def extract_features(model_backbone, data_loader, device):\n",
    "    \"\"\" Global Features  (pooling FPN)\"\"\"\n",
    "    all_features = []\n",
    "\n",
    "    # Valuationn mode\n",
    "    model_backbone.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, paths in tqdm(data_loader, desc=\"Extracting Features\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "\n",
    "            # Forward pass for backbone\n",
    "            features = model_backbone(images[0].unsqueeze(0)) \n",
    "\n",
    "            if isinstance(features, torch.Tensor):\n",
    "                features = OrderedDict([(\"0\", features)])\n",
    "\n",
    "            # Global pooled features\n",
    "            pooled = [\n",
    "                F.adaptive_avg_pool2d(f, 1).flatten(1)\n",
    "                for f in features.values()\n",
    "            ]\n",
    "            im_feat = torch.cat(pooled, dim=1)\n",
    "\n",
    "            all_features.append(im_feat.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(all_features, axis=0)\n",
    "\n",
    "#  Execution and visualization\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    BASE_DATA_PATH = '/content/data_extracted'\n",
    "    REAL_IMG_DIR = os.path.join(BASE_DATA_PATH, 'real_LRDD/train/images') # Target\n",
    "    VIRT_IMG_DIR = os.path.join(BASE_DATA_PATH, 'virtualonlymountains')  # Source\n",
    "\n",
    "    # Parametri\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    SAMPLE_SIZE = 1000 \n",
    "\n",
    "    #  Backbone (COCO weights)\n",
    "    weights = FasterRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "    base_model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "    backbone = base_model.backbone.to(device)\n",
    "\n",
    "    #  Loading Data\n",
    "    virtual_dataset = SimpleImageDataset(VIRT_IMG_DIR)\n",
    "    real_dataset = SimpleImageDataset(REAL_IMG_DIR)\n",
    "\n",
    "    # VSampling for T-SNE\n",
    "    if len(virtual_dataset) > SAMPLE_SIZE:\n",
    "        virtual_indices = np.random.choice(len(virtual_dataset), SAMPLE_SIZE, replace=False)\n",
    "        virtual_subset = torch.utils.data.Subset(virtual_dataset, virtual_indices)\n",
    "    else:\n",
    "        virtual_subset = virtual_dataset\n",
    "\n",
    "    if len(real_dataset) > SAMPLE_SIZE:\n",
    "        real_indices = np.random.choice(len(real_dataset), SAMPLE_SIZE, replace=False)\n",
    "        real_subset = torch.utils.data.Subset(real_dataset, real_indices)\n",
    "    else:\n",
    "        real_subset = real_dataset\n",
    "\n",
    "    virtual_loader = DataLoader(virtual_subset, batch_size=1, shuffle=False)\n",
    "    real_loader = DataLoader(real_subset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Extraction Features\n",
    "    print(\"Extraction features Sources...\")\n",
    "    virt_features = extract_features(backbone, virtual_loader, device)\n",
    "\n",
    "    print(\" Extraction features Target...\")\n",
    "    real_features = extract_features(backbone, real_loader, device)\n",
    "\n",
    "    # Combine features and build labels for domain (0=Virtual, 1=Real)\n",
    "    all_features = np.vstack([virt_features, real_features])\n",
    "    domain_labels = np.array([0] * len(virt_features) + [1] * len(real_features))\n",
    "\n",
    "    #   Dimensionality reduction with T-SNE ---\n",
    "    print(f\"Execution T-SNE on {len(all_features)} samples...\")\n",
    "    \n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000, learning_rate=200)\n",
    "    tsne_results = tsne.fit_transform(all_features)\n",
    "\n",
    "    # Visual\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Plot \n",
    "    plt.scatter(\n",
    "        tsne_results[domain_labels == 0, 0],\n",
    "        tsne_results[domain_labels == 0, 1],\n",
    "        label='Sources (Virtual)',\n",
    "        alpha=0.5,\n",
    "        color='blue'\n",
    "    )\n",
    "\n",
    "    # Plot \n",
    "    plt.scatter(\n",
    "        tsne_results[domain_labels == 1, 0],\n",
    "        tsne_results[domain_labels == 1, 1],\n",
    "        label='Target (Real)',\n",
    "        alpha=0.5,\n",
    "        color='red'\n",
    "    )\n",
    "\n",
    "    plt.title(\"Analysis T-SNE \")\n",
    "    plt.xlabel(\"Component T-SNE 1\")\n",
    "    plt.ylabel(\"Component T-SNE 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Save\n",
    "    plt.savefig('tsne_domain_shift.png')\n",
    "    plt.show()\n",
    "    print(\"T-SNE completed \")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN436p/xkhkV94jfLRL+0KF",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "189ea337ea4e488088bc36b5535dccfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fcde33f0157742b3bb145a11047b4cd1",
       "IPY_MODEL_61a0f4c028274845872bb2c67f388f83",
       "IPY_MODEL_f80b711abe644ebfa1cef82191e263b0"
      ],
      "layout": "IPY_MODEL_e53c373301ba4a72a9d34692b8909ceb"
     }
    },
    "28afef4df1494d7780272115b8425760": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32694a5b753a46db88ace75fced4de60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "61a0f4c028274845872bb2c67f388f83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8336df6bfcb457898873b6d9467481a",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7bf8e3f52cb5429ebfdde1d50cfef0db",
      "value": 10
     }
    },
    "6eca9acfd5a94e42a8b82e2e8ed9dfe5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bf8e3f52cb5429ebfdde1d50cfef0db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bcb590872e904198b9a2d4c03ee53a99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8336df6bfcb457898873b6d9467481a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e53c373301ba4a72a9d34692b8909ceb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f80b711abe644ebfa1cef82191e263b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6eca9acfd5a94e42a8b82e2e8ed9dfe5",
      "placeholder": "​",
      "style": "IPY_MODEL_bcb590872e904198b9a2d4c03ee53a99",
      "value": " 10/10 [00:04&lt;00:00,  2.92it/s]"
     }
    },
    "fcde33f0157742b3bb145a11047b4cd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28afef4df1494d7780272115b8425760",
      "placeholder": "​",
      "style": "IPY_MODEL_32694a5b753a46db88ace75fced4de60",
      "value": "Esecuzione Inferenza Veloce: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
