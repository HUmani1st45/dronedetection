{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079a4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from collections import OrderedDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3f462",
   "metadata": {},
   "source": [
    "gradient reversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca202be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\test_W0_T0_F60_S05_000009.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\test_W1_T2_F75_S120_000002.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\train_W0_T1_F60_S24_000007.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\train_W0_T1_F75_S28_000003.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\train_W0_T2_F60_S41_000006.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\train_W0_T3_F60_S59_000007.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\train_W2_T0_F60_S146_000005.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\train_W2_T1_F60_S163_000006.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\W0_T0_F60_S03_000008.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\W0_T0_F60_S06_000007.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\W0_T1_F60_S24_000009.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\W0_T1_F75_S27_000003.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\W0_T2_F60_S39_000002.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\W0_T3_F60_S57_000002.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\W1_T1_F75_S99_000003.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\W1_T2_F75_S117_000003.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\W2_T2_F90_S193_000001.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\W3_T0_F90_S229_000001.jpg\n",
      "Invalid boxes â†’ DELETING: C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\\W3_T3_F90_S283_000001.jpg\n",
      "\n",
      "ðŸ”¥ Deleted 19 corrupted images & labels.\n"
     ]
    }
   ],
   "source": [
    "deleted = 0\n",
    "\n",
    "for fname in os.listdir(img_dir):\n",
    "    if not fname.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        continue\n",
    "\n",
    "    img_path = os.path.join(img_dir, fname)\n",
    "    lbl_path = os.path.join(lbl_dir, os.path.splitext(fname)[0] + \".txt\")\n",
    "\n",
    "    # If label missing â†’ delete image\n",
    "    if not os.path.exists(lbl_path):\n",
    "        print(\"Missing label â†’ DELETING IMAGE:\", img_path)\n",
    "        try:\n",
    "            os.remove(img_path)\n",
    "        except Exception as e:\n",
    "            print(\"Could not delete (in use):\", e)\n",
    "        deleted += 1\n",
    "        continue\n",
    "\n",
    "    # Load image size SAFELY (auto-close)\n",
    "    try:\n",
    "        with Image.open(img_path) as img:\n",
    "            w, h = img.size\n",
    "    except Exception as e:\n",
    "        print(\"Unreadable image â†’ DELETING:\", img_path)\n",
    "        try:\n",
    "            os.remove(img_path)\n",
    "            if os.path.exists(lbl_path):\n",
    "                os.remove(lbl_path)\n",
    "        except Exception as e2:\n",
    "            print(\"Delete failed:\", e2)\n",
    "        deleted += 1\n",
    "        continue\n",
    "\n",
    "    # Load labels\n",
    "    try:\n",
    "        boxes = load_yolo_label(lbl_path, w, h)\n",
    "    except Exception as e:\n",
    "        print(\"Invalid label â†’ deleting:\", lbl_path)\n",
    "        os.remove(lbl_path)\n",
    "        try:\n",
    "            os.remove(img_path)\n",
    "        except:\n",
    "            pass\n",
    "        deleted += 1\n",
    "        continue\n",
    "\n",
    "    # Check if invalid\n",
    "    if is_invalid(boxes, w, h):\n",
    "        print(\"Invalid boxes â†’ DELETING:\", img_path)\n",
    "        try:\n",
    "            os.remove(img_path)\n",
    "            if os.path.exists(lbl_path):\n",
    "                os.remove(lbl_path)\n",
    "        except Exception as e:\n",
    "            print(\"Could not delete (in use):\", e)\n",
    "        deleted += 1\n",
    "\n",
    "print(f\"\\nðŸ”¥ Deleted {deleted} corrupted images & labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29cfc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# --- 1. The Autograd Function (No Change Needed Here) ---\n",
    "class GradientReverseFn(autograd.Function):\n",
    "    \"\"\"Handles the forward (identity) and backward (gradient reversal) pass.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_ # Save the dynamic lambda for the backward pass\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Reverse and scale gradient using the lambda saved in the context\n",
    "        return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "\n",
    "# --- 2. The Module Wrapper (Refined to accept lambda in forward) ---\n",
    "class GradientReverse(nn.Module):\n",
    "    \"\"\"\n",
    "    Module wrapper for GRL, now designed to accept the dynamic lambda\n",
    "    value in its forward call, which is passed down to the function.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, lambda_):\n",
    "        # We pass the dynamically calculated lambda_ directly to the function\n",
    "        return GradientReverseFn.apply(x, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a1df7",
   "metadata": {},
   "source": [
    "DA faster R-CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14f66d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# Import the refined GRL from the file above\n",
    "# Assuming GradientReverse is available in the scope\n",
    "\n",
    "class DAFasterRCNN_Global_Stable(nn.Module):\n",
    "    # Pass current_step directly, or use a separate set_current_step method\n",
    "    def __init__(self, num_classes, lambda_da_max=0.01, da_warmup_steps=1000): \n",
    "        super().__init__()\n",
    "        self.lambda_da_max = lambda_da_max\n",
    "        # Using 'steps' (iterations) is often better than 'epochs' for scheduling\n",
    "        self.da_warmup_steps = da_warmup_steps \n",
    "        self.current_step = 0 # Track current iteration internally\n",
    "\n",
    "        # ----------- BASE DETECTOR -------------\n",
    "        # Use small anchors for tiny drones\n",
    "        anchor_generator = AnchorGenerator(\n",
    "            sizes=((8, 16, 32), (16, 32, 64), (32, 64, 128), (64, 128, 256), (128, 256, 512)),\n",
    "            aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    "        )\n",
    "\n",
    "        self.detector = fasterrcnn_resnet50_fpn(\n",
    "            weights=None,\n",
    "            num_classes=num_classes,\n",
    "            rpn_anchor_generator=anchor_generator\n",
    "        )\n",
    "\n",
    "        # GRL initialization (no lambda_ required now)\n",
    "        self.grl = GradientReverse()\n",
    "\n",
    "        # Domain classifier (lazy init)\n",
    "        self.im_domain_head = None\n",
    "\n",
    "    def set_current_step(self, step):\n",
    "        \"\"\"Allows external update of the current iteration/step count.\"\"\"\n",
    "        self.current_step = step\n",
    "\n",
    "    def warmup_lambda(self):\n",
    "        \"\"\"\n",
    "        Gradually increase lambda_da over first steps.\n",
    "        \"\"\"\n",
    "        if self.current_step >= self.da_warmup_steps:\n",
    "            return self.lambda_da_max\n",
    "        \n",
    "        # Use a smooth function like p = 2 / (1 + exp(-gamma * iter)) - 1\n",
    "        # Here, a simple linear ramp is used based on your initial logic\n",
    "        progress = self.current_step / self.da_warmup_steps\n",
    "        return progress * self.lambda_da_max\n",
    "\n",
    "    def _build_domain_head(self, feature_dict):\n",
    "        num_scales = len(feature_dict)\n",
    "        # Assuming FPN features are P2-P6 (5 scales), and dim is 256\n",
    "        feat_dim = next(iter(feature_dict.values())).shape[1]\n",
    "        input_dim = num_scales * feat_dim\n",
    "\n",
    "        self.im_domain_head = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        ).to(next(self.parameters()).device)\n",
    "\n",
    "    def forward(self, images, targets=None, domain=None):\n",
    "        if self.training and domain is None:\n",
    "            raise ValueError(\"Need domain='source' or 'target' during training.\")\n",
    "\n",
    "        device = images[0].device\n",
    "        original_sizes = [img.shape[-2:] for img in images]\n",
    "\n",
    "        # --- Transform ---\n",
    "        images_t, targets_t = self.detector.transform(images, targets)\n",
    "\n",
    "        # --- Backbone ---\n",
    "        features = self.detector.backbone(images_t.tensors)\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = OrderedDict([(\"0\", features)])\n",
    "\n",
    "        # Build domain head lazily\n",
    "        if self.im_domain_head is None:\n",
    "            self._build_domain_head(features)\n",
    "\n",
    "        # --- DETECTION (SOURCE ONLY) ---\n",
    "        det_losses = {}\n",
    "        detections = None\n",
    "        \n",
    "        # Only compute RPN/ROI losses if it's the source domain\n",
    "        if domain == \"source\":\n",
    "            # The standard torchvision implementation handles the loss computation internally\n",
    "            # when targets are provided.\n",
    "            if self.training:\n",
    "                # In training mode, call the detector with targets to get losses\n",
    "                det_outputs = self.detector.forward(images, targets)\n",
    "                # The detector's forward returns losses when targets is not None in training\n",
    "                det_losses.update(det_outputs)\n",
    "            else:\n",
    "                # Inference mode (done in the final else block for simplicity)\n",
    "                pass\n",
    "\n",
    "\n",
    "        # --- DOMAIN ADAPTATION LOSS ---\n",
    "        da_losses = {}\n",
    "        if self.training:\n",
    "            # Domain label (0=source, 1=target)\n",
    "            dom_label = 0 if domain == \"source\" else 1\n",
    "            dom_label_tensor = torch.full(\n",
    "                (len(images),), dom_label, dtype=torch.long, device=device\n",
    "            )\n",
    "\n",
    "            # Global pooled features (same as your original logic - correct)\n",
    "            pooled = [\n",
    "                F.adaptive_avg_pool2d(f, 1).flatten(1)\n",
    "                for f in features.values()\n",
    "            ]\n",
    "            im_feat = torch.cat(pooled, dim=1)\n",
    "\n",
    "            # GRL with warm-up\n",
    "            lambda_da = self.warmup_lambda()\n",
    "            # Pass the dynamic lambda_da to the GRL's forward method\n",
    "            rev_feat = self.grl(im_feat, lambda_da)\n",
    "\n",
    "            logits = self.im_domain_head(rev_feat)\n",
    "            da_losses[\"loss_da_im\"] = F.cross_entropy(logits, dom_label_tensor)\n",
    "\n",
    "        if self.training:\n",
    "            losses = {}\n",
    "            losses.update(det_losses)\n",
    "            losses.update(da_losses)\n",
    "            return losses\n",
    "        else:\n",
    "            # Inference: Detector forward returns detections when targets is None\n",
    "            # We call the standard detector forward here for clean inference.\n",
    "            # NOTE: We can't reuse features/proposals easily without replicating\n",
    "            # the internal logic of the torchvision model. The cleanest way \n",
    "            # is to call the full detector forward for inference.\n",
    "            self.detector.eval()\n",
    "            with torch.no_grad():\n",
    "                # Pass original images (not transformed) to the detector's forward method\n",
    "                # which handles transformation and postprocessing internally.\n",
    "                detections = self.detector(images)\n",
    "            self.detector.train() # Reset to train mode if we are inside a training loop batch\n",
    "            return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199405ef",
   "metadata": {},
   "source": [
    "dataset for faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9942ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloTxtDetectionDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, label_dir: str, transforms=None):\n",
    "        \"\"\"\n",
    "        img_dir: folder with images\n",
    "        label_dir: folder with YOLO txt labels\n",
    "        transforms: callable(img, target) -> (img, target)\n",
    "        \"\"\"\n",
    "        self.img_paths = sorted(\n",
    "            [p for p in glob(os.path.join(img_dir, \"*\")) if p.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
    "        )\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def _load_yolo_labels(self, img_path: str, w: int, h: int):\n",
    "        # Get corresponding label path\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        label_path = os.path.join(self.label_dir, base + \".txt\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        if not os.path.exists(label_path):\n",
    "            # No labels: return empty\n",
    "            return torch.empty((0, 4), dtype=torch.float32), torch.empty((0,), dtype=torch.int64)\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                cls = int(parts[0])\n",
    "                x_c, y_c, bw, bh = map(float, parts[1:5])\n",
    "\n",
    "                # Convert from normalized cx, cy, w, h â†’ absolute xyxy\n",
    "                x_c *= w\n",
    "                y_c *= h\n",
    "                bw *= w\n",
    "                bh *= h\n",
    "                x1 = x_c - bw / 2\n",
    "                y1 = y_c - bh / 2\n",
    "                x2 = x_c + bw / 2\n",
    "                y2 = y_c + bh / 2\n",
    "\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                labels.append(cls)  # map as needed (e.g. single class \"drone\" = 1)\n",
    "\n",
    "        if boxes:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.empty((0, 4), dtype=torch.float32)\n",
    "            labels = torch.empty((0,), dtype=torch.int64)\n",
    "\n",
    "        return boxes, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        boxes, labels = self._load_yolo_labels(img_path, w, h)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) if boxes.numel() > 0 else torch.tensor([]),\n",
    "            \"iscrowd\": torch.zeros((boxes.shape[0],), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        # Convert PIL â†’ Tensor here if you want:\n",
    "        img = TF.to_tensor(img)  # [C,H,W], float32 in [0,1]\n",
    "        return img, target\n",
    "\n",
    "\n",
    "def det_collate(batch):\n",
    "    imgs, targets = zip(*batch)\n",
    "    return list(imgs), list(targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c6502",
   "metadata": {},
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfde2975",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_img_dir = r\"C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\real\\train\\images\"\n",
    "real_lbl_dir = r\"C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\real\\train\\labels\"\n",
    "\n",
    "virt_img_dir = r\"C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\images\"\n",
    "virt_lbl_dir = r\"C:\\Users\\giorg\\Downloads\\drone-detection-distance-template\\drone-detection-distance\\data\\virtual\\train\\labels\"\n",
    "\n",
    "source_dataset = YoloTxtDetectionDataset(virt_img_dir, virt_lbl_dir)\n",
    "target_dataset = YoloTxtDetectionDataset(real_img_dir, real_lbl_dir)  # labels ignored in training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a61513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import itertools\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes = 2  # e.g. background + 1 drone class -> check your IDs\n",
    "da_weight = 0.001  # changed from 0.01 it was exploding\n",
    "lambda_da_max = 1   # how strong to weight DA loss vs detection\n",
    "warmup_steps = 1000 \n",
    "\n",
    "model = DAFasterRCNN_Global_Stable(\n",
    "    num_classes=num_classes,\n",
    "    lambda_da_max=lambda_da_max,\n",
    "    da_warmup_steps=warmup_steps\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.001,                  # ðŸ”¥ lower LR for stability (very important)\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "source_loader = DataLoader(\n",
    "    source_dataset, batch_size=2, shuffle=True, collate_fn=det_collate\n",
    ")\n",
    "target_loader = DataLoader(\n",
    "    target_dataset, batch_size=2, shuffle=True, collate_fn=det_collate\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils as utils\n",
    "import itertools\n",
    "\n",
    "def train_da_stable(\n",
    "    model,\n",
    "    source_loader,\n",
    "    target_loader,\n",
    "    optimizer,\n",
    "    num_epochs=10,\n",
    "    da_weight=0.001,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    model.train()\n",
    "    target_iter = itertools.cycle(target_loader)\n",
    "    global_step = 0 \n",
    "    \n",
    "    # Initialize total_loss outside the epoch loop, \n",
    "    # to be safely used in the checkpoint logic on the first epoch.\n",
    "    total_loss = torch.tensor(0.0, device=device) \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # -----------------------------------------------\n",
    "        # 1. BATCH LOOP (INNER LOOP)\n",
    "        # -----------------------------------------------\n",
    "        for i, (src_imgs, src_tgts) in enumerate(source_loader):\n",
    "            \n",
    "            # Update the global step and the model's internal step counter\n",
    "            global_step += 1\n",
    "            model.set_current_step(global_step)\n",
    "\n",
    "            src_imgs = [img.to(device) for img in src_imgs]\n",
    "            src_tgts = [{k: v.to(device) for k, v in t.items()} for t in src_tgts]\n",
    "\n",
    "            # --- SOURCE FORWARD PASS (Task Loss + DA Loss) ---\n",
    "            src_loss_dict = model(src_imgs, src_tgts, domain=\"source\")\n",
    "\n",
    "            det_loss = (\n",
    "                src_loss_dict.get(\"loss_classifier\", torch.tensor(0.0, device=device)) +\n",
    "                src_loss_dict.get(\"loss_box_reg\", torch.tensor(0.0, device=device)) +\n",
    "                src_loss_dict.get(\"loss_objectness\", torch.tensor(0.0, device=device)) +\n",
    "                src_loss_dict.get(\"loss_rpn_box_reg\", torch.tensor(0.0, device=device))\n",
    "            )\n",
    "            da_src = src_loss_dict.get(\"loss_da_im\", torch.tensor(0.0, device=device))\n",
    "\n",
    "            # --- TARGET FORWARD PASS (DA Loss ONLY) ---\n",
    "            tgt_imgs, _ = next(target_iter)\n",
    "            tgt_imgs = [img.to(device) for img in tgt_imgs]\n",
    "\n",
    "            tgt_loss_dict = model(tgt_imgs, targets=None, domain=\"target\")\n",
    "            da_tgt = tgt_loss_dict.get(\"loss_da_im\", torch.tensor(0.0, device=device))\n",
    "\n",
    "            # --- TOTAL LOSS & OPTIMIZATION ---\n",
    "            # Total Loss = Task Loss + DA Weight * (DA Source Loss + DA Target Loss)\n",
    "            total_loss = det_loss + da_weight * (da_src + da_tgt)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Gradient clipping is essential for adversarial stability\n",
    "            utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            # --- LOGGING ---\n",
    "            if global_step % 10 == 0:\n",
    "                current_lambda = model.warmup_lambda()\n",
    "                print(\n",
    "                    f\"[Epoch {epoch} Step {global_step}] \"\n",
    "                    f\"det={det_loss.item():.4f} \"\n",
    "                    f\"DA_src={da_src.item():.4f} \"\n",
    "                    f\"DA_tgt={da_tgt.item():.4f} \"\n",
    "                    f\"lambda={current_lambda:.6f} \"\n",
    "                    f\"total={total_loss.item():.4f}\"\n",
    "                )\n",
    "        \n",
    "        # -----------------------------------------------\n",
    "        # 2. CHECKPOINT SAVING (CORRECT PLACEMENT)\n",
    "        #    This runs once, after all batches in the current epoch are complete.\n",
    "        # -----------------------------------------------\n",
    "        if epoch % 1 == 0: # Save every epoch\n",
    "            save_path = f\"da_frcnn_epoch_{epoch}.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'global_step': global_step, # Crucial for resuming lambda schedule\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': total_loss.item(), # Last calculated loss of this epoch\n",
    "            }, save_path)\n",
    "            print(f\"\\nModel checkpoint saved to {save_path}\\n\")\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "\n",
    "# Call training:\n",
    "#train_da_stable(model, source_loader, target_loader, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb58fd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Step 10] det=0.7699 DA_src=0.7602 DA_tgt=0.6704 lambda=0.010000 total=0.7713\n",
      "[Epoch 0 Step 20] det=0.6884 DA_src=0.7575 DA_tgt=0.6658 lambda=0.020000 total=0.6898\n",
      "[Epoch 0 Step 30] det=0.2815 DA_src=0.9188 DA_tgt=0.5210 lambda=0.030000 total=0.2829\n",
      "[Epoch 0 Step 40] det=0.0570 DA_src=1.8915 DA_tgt=0.2334 lambda=0.040000 total=0.0591\n",
      "[Epoch 0 Step 50] det=0.7231 DA_src=2.0333 DA_tgt=0.1388 lambda=0.050000 total=0.7253\n",
      "[Epoch 0 Step 60] det=0.1527 DA_src=2.0868 DA_tgt=0.1036 lambda=0.060000 total=0.1549\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_da_stable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mda_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 67\u001b[0m, in \u001b[0;36mtrain_da_stable\u001b[1;34m(model, source_loader, target_loader, optimizer, num_epochs, da_weight, device)\u001b[0m\n\u001b[0;32m     64\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39mset_current_step(global_step)\n\u001b[1;32m---> 67\u001b[0m src_imgs \u001b[38;5;241m=\u001b[39m [img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m src_imgs]\n\u001b[0;32m     68\u001b[0m src_tgts \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m src_tgts]\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# --- SOURCE FORWARD PASS (Task Loss + DA Loss) ---\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     64\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39mset_current_step(global_step)\n\u001b[1;32m---> 67\u001b[0m src_imgs \u001b[38;5;241m=\u001b[39m [\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m src_imgs]\n\u001b[0;32m     68\u001b[0m src_tgts \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m src_tgts]\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# --- SOURCE FORWARD PASS (Task Loss + DA Loss) ---\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_da_stable(model, source_loader, target_loader, optimizer, num_epochs=10, da_weight=da_weight, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf33d10a",
   "metadata": {},
   "source": [
    "## Check corrupted images/labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50ee79e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative box coordinate at index: 26\n",
      "Negative box coordinate at index: 38\n",
      "Negative box coordinate at index: 148\n",
      "Negative box coordinate at index: 159\n",
      "Negative box coordinate at index: 337\n",
      "Negative box coordinate at index: 595\n",
      "Negative box coordinate at index: 676\n",
      "Negative box coordinate at index: 1099\n",
      "Negative box coordinate at index: 1308\n",
      "Negative box coordinate at index: 1604\n",
      "Negative box coordinate at index: 1682\n",
      "Invalid box size at index: 1835\n",
      "Invalid box size at index: 2046\n",
      "Invalid box size at index: 2066\n",
      "Invalid box size at index: 2136\n",
      "Invalid box size at index: 2702\n",
      "Invalid box size at index: 2793\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(source_dataset)):\n",
    "    try:\n",
    "        img, tgt = source_dataset[idx]\n",
    "        boxes = tgt[\"boxes\"]\n",
    "\n",
    "        if torch.isnan(boxes).any():\n",
    "            print(\"NaN boxes at index:\", idx)\n",
    "\n",
    "        if (boxes[:,2] <= boxes[:,0]).any() or (boxes[:,3] <= boxes[:,1]).any():\n",
    "            print(\"Invalid box size at index:\", idx)\n",
    "\n",
    "        if (boxes < 0).any():\n",
    "            print(\"Negative box coordinate at index:\", idx)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error at index:\", idx, \"->\", str(e))\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drone-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
